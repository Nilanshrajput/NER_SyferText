{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lSTM_colab.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPktiJAt81/M00CI70T0wF/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nilanshrajput/NER_SyferText/blob/master/lSTM_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBGOlkdKCP9O",
        "colab_type": "code",
        "outputId": "1f9ba176-cc69-46a7-e192-ba2ed07e4588",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "!git clone https://github.com/synalp/NER.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'NER'...\n",
            "remote: Enumerating objects: 3148, done.\u001b[K\n",
            "remote: Total 3148 (delta 0), reused 0 (delta 0), pack-reused 3148\u001b[K\n",
            "Receiving objects: 100% (3148/3148), 281.51 MiB | 12.78 MiB/s, done.\n",
            "Resolving deltas: 100% (2066/2066), done.\n",
            "Checking out files: 100% (189/189), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lW7VtQ2yCiPL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import torch\n",
        "from torchtext import data\n",
        "from torchtext.datasets import SequenceTaggingDataset, CoNLL2000Chunking\n",
        "from torchtext.vocab import Vectors, GloVe, CharNGram\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import logging\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "def conll2003_dataset(tag_type, batch_size, root='/content/NER/corpus/CoNLL-2003', \n",
        "                          train_file='eng.train', \n",
        "                          validation_file='eng.testa',\n",
        "                          test_file='eng.testb',\n",
        "                          convert_digits=True):\n",
        "    \"\"\"\n",
        "    conll2003: Conll 2003 (Parser only. You must place the files)\n",
        "    Extract Conll2003 dataset using torchtext. Applies GloVe 6B.200d and Char N-gram\n",
        "    pretrained vectors. Also sets up per word character Field\n",
        "    Parameters:\n",
        "        tag_type: Type of tag to pick as task [pos, chunk, ner]\n",
        "        batch_size: Batch size to return from iterator\n",
        "        root: Dataset root directory\n",
        "        train_file: Train filename\n",
        "        validation_file: Validation filename\n",
        "        test_file: Test filename\n",
        "        convert_digits: If True will convert numbers to single 0's\n",
        "    Returns:\n",
        "        A dict containing:\n",
        "            task: 'conll2003.' + tag_type\n",
        "            iters: (train iter, validation iter, test iter)\n",
        "            vocabs: (Inputs word vocabulary, Inputs character vocabulary, \n",
        "                    Tag vocabulary )\n",
        "    \"\"\"\n",
        "    \n",
        "    # Setup fields with batch dimension first\n",
        "    inputs_word = data.Field(init_token=\"<bos>\", eos_token=\"<eos>\",fix_length=100, batch_first=True, lower=True)\n",
        "\n",
        "    inputs_char_nesting = data.Field(tokenize=list, init_token=\"<bos>\", eos_token=\"<eos>\", \n",
        "                                    batch_first=True)\n",
        "\n",
        "    inputs_char = data.NestedField(inputs_char_nesting, \n",
        "                                    init_token=\"<bos>\", eos_token=\"<eos>\")\n",
        "                        \n",
        "\n",
        "    labels = data.Field(init_token=\"<bos>\", eos_token=\"<eos>\", fix_length=50, batch_first=True)\n",
        "\n",
        "    fields = ([(('inputs_word', 'inputs_char'), (inputs_word, inputs_char))] + \n",
        "                [('labels', labels) if label == tag_type else (None, None) \n",
        "                for label in ['pos', 'chunk', 'ner']])\n",
        "\n",
        "    # Load the data\n",
        "    train, val, test = SequenceTaggingDataset.splits(\n",
        "                                path=root, \n",
        "                                train=train_file, \n",
        "                                validation=validation_file, \n",
        "                                test=test_file,\n",
        "                                separator=' ',\n",
        "                                fields=tuple(fields))\n",
        "\n",
        "\n",
        "    \n",
        "    # Build vocab\n",
        "    inputs_char.build_vocab(train.inputs_char, val.inputs_char, test.inputs_char)\n",
        "    inputs_word.build_vocab(train.inputs_word, val.inputs_word, test.inputs_word, max_size=50000,\n",
        "                        vectors=[GloVe(name='6B', dim='300'), CharNGram()])\n",
        "    \n",
        "    labels.build_vocab(train.labels)\n",
        "  \n",
        "\n",
        "    # Get iterators\n",
        "    train_iter, val_iter, test_iter = data.BucketIterator.splits(\n",
        "                            (train, val, test), batch_size=batch_size, \n",
        "                            device=torch.device(\"cpu\"))\n",
        "    train_iter.repeat = False\n",
        "    \n",
        "    return {\n",
        "        'task': 'conll2003.%s'%tag_type,\n",
        "        'iters': (train_iter, val_iter, test_iter), \n",
        "        'vocabs': (inputs_word.vocab, inputs_char.vocab, labels.vocab) \n",
        "        }\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPfLM2mGCwqP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "39ed8f2c-f66e-477d-cba9-ae55c25c82b5"
      },
      "source": [
        "dic = conll2003_dataset('ner', batch_size = 64)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [06:30, 2.21MB/s]                           \n",
            "100%|█████████▉| 399741/400000 [00:50<00:00, 7657.63it/s]\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz: 0.00B [00:00, ?B/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:   0%|          | 8.19k/956M [00:03<113:58:40, 2.33kB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:   0%|          | 172k/956M [00:03<79:49:38, 3.32kB/s]  \u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:   0%|          | 1.50M/956M [00:03<55:48:30, 4.75kB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:   1%|          | 7.95M/956M [00:03<38:48:11, 6.78kB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:   2%|▏         | 14.7M/956M [00:03<26:58:15, 9.69kB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:   2%|▏         | 22.6M/956M [00:04<18:43:16, 13.8kB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:   3%|▎         | 27.0M/956M [00:04<13:02:43, 19.8kB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:   4%|▎         | 34.4M/956M [00:04<9:03:35, 28.2kB/s] \u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:   4%|▍         | 41.4M/956M [00:04<6:17:40, 40.3kB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:   5%|▌         | 48.9M/956M [00:04<4:22:16, 57.6kB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:   6%|▌         | 56.4M/956M [00:04<3:02:07, 82.3kB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:   7%|▋         | 63.5M/956M [00:04<2:06:32, 117kB/s] \u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:   7%|▋         | 71.1M/956M [00:04<1:27:52, 168kB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:   8%|▊         | 78.8M/956M [00:04<1:01:02, 239kB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:   9%|▉         | 86.1M/956M [00:04<42:26, 341kB/s]  \u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  10%|▉         | 93.7M/956M [00:05<29:30, 487kB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  11%|█         | 101M/956M [00:05<20:31, 694kB/s] \u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  11%|█▏        | 108M/956M [00:05<14:18, 987kB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  12%|█▏        | 116M/956M [00:05<09:59, 1.40MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  13%|█▎        | 121M/956M [00:05<07:02, 1.98MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  13%|█▎        | 127M/956M [00:05<04:56, 2.79MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  14%|█▍        | 135M/956M [00:05<03:29, 3.92MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  15%|█▍        | 142M/956M [00:05<02:28, 5.48MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  16%|█▌        | 150M/956M [00:05<01:46, 7.59MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  16%|█▋        | 157M/956M [00:05<01:17, 10.4MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  17%|█▋        | 164M/956M [00:06<00:56, 14.0MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  18%|█▊        | 172M/956M [00:06<00:42, 18.5MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  19%|█▉        | 179M/956M [00:06<00:32, 23.9MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  20%|█▉        | 186M/956M [00:06<00:25, 29.8MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  20%|██        | 194M/956M [00:06<00:20, 36.5MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  21%|██        | 202M/956M [00:06<00:17, 43.1MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  22%|██▏       | 209M/956M [00:06<00:14, 49.8MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  23%|██▎       | 217M/956M [00:06<00:13, 55.2MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  24%|██▎       | 225M/956M [00:06<00:12, 60.5MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  24%|██▍       | 232M/956M [00:06<00:11, 64.1MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  25%|██▌       | 240M/956M [00:07<00:10, 66.9MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  26%|██▌       | 247M/956M [00:07<00:10, 69.4MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  27%|██▋       | 255M/956M [00:07<00:09, 71.3MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  27%|██▋       | 262M/956M [00:07<00:09, 72.2MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  28%|██▊       | 270M/956M [00:07<00:09, 73.6MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  29%|██▉       | 278M/956M [00:07<00:09, 73.9MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  30%|██▉       | 284M/956M [00:07<00:09, 71.3MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  30%|███       | 289M/956M [00:07<00:10, 63.2MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  31%|███       | 296M/956M [00:07<00:10, 65.6MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  32%|███▏      | 304M/956M [00:07<00:09, 68.4MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  33%|███▎      | 311M/956M [00:08<00:09, 69.7MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  33%|███▎      | 319M/956M [00:08<00:09, 70.7MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  34%|███▍      | 326M/956M [00:08<00:08, 72.0MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  35%|███▍      | 333M/956M [00:08<00:08, 70.5MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  36%|███▌      | 341M/956M [00:08<00:08, 71.8MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  36%|███▋      | 348M/956M [00:08<00:08, 71.4MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  37%|███▋      | 355M/956M [00:08<00:08, 71.1MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  38%|███▊      | 362M/956M [00:08<00:08, 72.2MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  39%|███▊      | 370M/956M [00:08<00:08, 72.9MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  39%|███▉      | 377M/956M [00:08<00:07, 72.7MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  40%|████      | 385M/956M [00:09<00:07, 73.8MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  41%|████      | 392M/956M [00:09<00:07, 73.6MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  42%|████▏     | 399M/956M [00:09<00:07, 72.9MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  43%|████▎     | 407M/956M [00:09<00:07, 72.6MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  43%|████▎     | 414M/956M [00:09<00:07, 72.6MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  44%|████▍     | 421M/956M [00:09<00:07, 73.7MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  45%|████▍     | 429M/956M [00:09<00:07, 73.7MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  46%|████▌     | 437M/956M [00:09<00:06, 74.9MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  46%|████▋     | 444M/956M [00:09<00:06, 74.8MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  47%|████▋     | 451M/956M [00:09<00:07, 71.3MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  48%|████▊     | 458M/956M [00:10<00:06, 71.3MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  48%|████▊     | 462M/956M [00:10<00:08, 59.1MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  49%|████▊     | 465M/956M [00:10<00:10, 46.3MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  49%|████▉     | 470M/956M [00:10<00:11, 44.1MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  50%|████▉     | 475M/956M [00:10<00:10, 45.8MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  50%|█████     | 479M/956M [00:10<00:10, 44.1MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  51%|█████     | 484M/956M [00:10<00:10, 44.7MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  51%|█████     | 489M/956M [00:10<00:10, 46.4MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  52%|█████▏    | 493M/956M [00:10<00:10, 43.9MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  52%|█████▏    | 498M/956M [00:11<00:10, 44.9MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  53%|█████▎    | 502M/956M [00:11<00:10, 43.9MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  53%|█████▎    | 507M/956M [00:11<00:09, 46.0MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  54%|█████▎    | 512M/956M [00:11<00:09, 45.1MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  54%|█████▍    | 517M/956M [00:11<00:09, 46.9MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  55%|█████▍    | 521M/956M [00:11<00:09, 46.1MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  55%|█████▌    | 526M/956M [00:11<00:09, 47.3MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  56%|█████▌    | 531M/956M [00:11<00:09, 46.4MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  56%|█████▌    | 536M/956M [00:11<00:08, 47.3MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  57%|█████▋    | 541M/956M [00:11<00:08, 48.0MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  57%|█████▋    | 546M/956M [00:12<00:08, 47.5MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  58%|█████▊    | 551M/956M [00:12<00:08, 47.4MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  58%|█████▊    | 556M/956M [00:12<00:08, 48.8MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  59%|█████▊    | 561M/956M [00:12<00:08, 47.8MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  59%|█████▉    | 566M/956M [00:12<00:07, 49.2MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  60%|█████▉    | 570M/956M [00:12<00:08, 48.0MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  60%|██████    | 576M/956M [00:12<00:07, 48.7MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  61%|██████    | 580M/956M [00:12<00:07, 48.7MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  61%|██████▏   | 585M/956M [00:12<00:07, 48.8MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  62%|██████▏   | 591M/956M [00:12<00:07, 49.7MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  62%|██████▏   | 595M/956M [00:13<00:07, 49.5MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  63%|██████▎   | 601M/956M [00:13<00:07, 50.3MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  63%|██████▎   | 606M/956M [00:13<00:07, 49.4MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  64%|██████▍   | 611M/956M [00:13<00:06, 50.6MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  64%|██████▍   | 616M/956M [00:13<00:06, 49.2MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  65%|██████▌   | 621M/956M [00:13<00:06, 51.0MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  66%|██████▌   | 626M/956M [00:13<00:06, 49.9MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  66%|██████▌   | 631M/956M [00:13<00:06, 51.2MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  67%|██████▋   | 636M/956M [00:13<00:06, 49.7MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  67%|██████▋   | 642M/956M [00:13<00:06, 51.3MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  68%|██████▊   | 646M/956M [00:14<00:06, 49.8MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  68%|██████▊   | 652M/956M [00:14<00:05, 51.6MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  69%|██████▊   | 657M/956M [00:14<00:05, 50.2MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  69%|██████▉   | 663M/956M [00:14<00:05, 52.2MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  70%|██████▉   | 667M/956M [00:14<00:05, 50.6MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  70%|███████   | 673M/956M [00:14<00:05, 51.3MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  71%|███████   | 678M/956M [00:14<00:05, 50.4MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  72%|███████▏  | 684M/956M [00:14<00:05, 52.5MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  72%|███████▏  | 688M/956M [00:14<00:05, 50.5MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  73%|███████▎  | 694M/956M [00:14<00:04, 52.7MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  73%|███████▎  | 699M/956M [00:15<00:05, 50.5MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  74%|███████▍  | 705M/956M [00:15<00:04, 53.0MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  74%|███████▍  | 709M/956M [00:15<00:04, 50.1MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  75%|███████▍  | 715M/956M [00:15<00:04, 52.6MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  75%|███████▌  | 720M/956M [00:15<00:04, 50.0MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  76%|███████▌  | 726M/956M [00:15<00:04, 52.4MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  76%|███████▋  | 730M/956M [00:15<00:04, 50.4MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  77%|███████▋  | 735M/956M [00:15<00:04, 50.7MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  77%|███████▋  | 740M/956M [00:15<00:04, 47.8MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  78%|███████▊  | 745M/956M [00:15<00:04, 47.5MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  79%|███████▊  | 751M/956M [00:16<00:04, 50.7MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  79%|███████▉  | 755M/956M [00:16<00:04, 49.0MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  80%|███████▉  | 761M/956M [00:16<00:03, 51.6MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  80%|████████  | 766M/956M [00:16<00:03, 50.2MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  81%|████████  | 771M/956M [00:16<00:03, 51.9MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  81%|████████  | 776M/956M [00:16<00:03, 50.3MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  82%|████████▏ | 782M/956M [00:16<00:03, 52.5MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  82%|████████▏ | 787M/956M [00:16<00:03, 51.0MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  83%|████████▎ | 793M/956M [00:16<00:03, 51.5MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  84%|████████▎ | 798M/956M [00:16<00:02, 53.1MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  84%|████████▍ | 803M/956M [00:17<00:02, 51.5MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  85%|████████▍ | 809M/956M [00:17<00:02, 53.4MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  85%|████████▌ | 814M/956M [00:17<00:02, 52.0MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  86%|████████▌ | 820M/956M [00:17<00:02, 53.5MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  86%|████████▋ | 825M/956M [00:17<00:02, 52.6MB/s]\u001b[A\n",
            "100%|█████████▉| 399741/400000 [01:10<00:00, 7657.63it/s]\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  87%|████████▋ | 835M/956M [00:17<00:02, 51.4MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  88%|████████▊ | 841M/956M [00:17<00:02, 53.2MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  89%|████████▊ | 846M/956M [00:17<00:02, 51.6MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  89%|████████▉ | 852M/956M [00:17<00:01, 53.5MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  90%|████████▉ | 856M/956M [00:18<00:01, 50.8MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  90%|█████████ | 862M/956M [00:18<00:01, 53.1MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  91%|█████████ | 867M/956M [00:18<00:01, 51.6MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  91%|█████████▏| 873M/956M [00:18<00:01, 53.3MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  92%|█████████▏| 878M/956M [00:18<00:01, 52.0MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  92%|█████████▏| 883M/956M [00:18<00:01, 52.7MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  93%|█████████▎| 887M/956M [00:18<00:01, 49.2MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  93%|█████████▎| 892M/956M [00:18<00:01, 48.1MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  94%|█████████▍| 897M/956M [00:18<00:01, 48.7MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  95%|█████████▍| 903M/956M [00:18<00:01, 51.1MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  95%|█████████▌| 908M/956M [00:19<00:00, 50.7MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  96%|█████████▌| 914M/956M [00:19<00:00, 52.4MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  96%|█████████▌| 919M/956M [00:19<00:00, 51.3MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  97%|█████████▋| 924M/956M [00:19<00:00, 52.5MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  97%|█████████▋| 930M/956M [00:19<00:00, 52.9MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  98%|█████████▊| 935M/956M [00:19<00:00, 53.1MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  98%|█████████▊| 940M/956M [00:19<00:00, 52.4MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz:  99%|█████████▉| 946M/956M [00:19<00:00, 52.7MB/s]\u001b[A\n",
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz: 956MB [00:19, 47.8MB/s]                           \n",
            "\n",
            "  0%|          | 0/874474 [00:00<?, ?it/s]\u001b[A\n",
            "  0%|          | 1535/874474 [00:00<00:56, 15345.37it/s]\u001b[A\n",
            "  0%|          | 3086/874474 [00:00<00:56, 15392.93it/s]\u001b[A\n",
            "  1%|          | 4763/874474 [00:00<00:55, 15780.75it/s]\u001b[A\n",
            "  1%|          | 6290/874474 [00:00<00:55, 15623.05it/s]\u001b[A\n",
            "  1%|          | 7981/874474 [00:00<00:54, 15985.84it/s]\u001b[A\n",
            "  1%|          | 9486/874474 [00:00<00:55, 15690.96it/s]\u001b[A\n",
            "  1%|▏         | 11162/874474 [00:00<00:53, 15995.13it/s]\u001b[A\n",
            "  1%|▏         | 12860/874474 [00:00<00:52, 16276.80it/s]\u001b[A\n",
            "  2%|▏         | 14571/874474 [00:00<00:52, 16516.44it/s]\u001b[A\n",
            "  2%|▏         | 16165/874474 [00:01<00:54, 15864.19it/s]\u001b[A\n",
            "  2%|▏         | 17717/874474 [00:01<00:54, 15753.87it/s]\u001b[A\n",
            "  2%|▏         | 19383/874474 [00:01<00:53, 16013.62it/s]\u001b[A\n",
            "  2%|▏         | 21112/874474 [00:01<00:52, 16375.48it/s]\u001b[A\n",
            "  3%|▎         | 22799/874474 [00:01<00:51, 16519.48it/s]\u001b[A\n",
            "  3%|▎         | 24460/874474 [00:01<00:51, 16545.23it/s]\u001b[A\n",
            "  3%|▎         | 26111/874474 [00:01<00:52, 16063.98it/s]\u001b[A\n",
            "  3%|▎         | 27814/874474 [00:01<00:51, 16340.95it/s]\u001b[A\n",
            "  3%|▎         | 29500/874474 [00:01<00:51, 16491.54it/s]\u001b[A\n",
            "  4%|▎         | 31187/874474 [00:01<00:50, 16600.54it/s]\u001b[A\n",
            "  4%|▍         | 32886/874474 [00:02<00:50, 16714.58it/s]\u001b[A\n",
            "  4%|▍         | 34559/874474 [00:02<00:51, 16236.92it/s]\u001b[A\n",
            "  4%|▍         | 36187/874474 [00:02<01:01, 13589.27it/s]\u001b[A\n",
            "  4%|▍         | 37844/874474 [00:02<00:58, 14363.97it/s]\u001b[A\n",
            "  5%|▍         | 39572/874474 [00:02<00:55, 15127.87it/s]\u001b[A\n",
            "  5%|▍         | 41266/874474 [00:02<00:53, 15629.01it/s]\u001b[A\n",
            "  5%|▍         | 42873/874474 [00:02<00:53, 15682.70it/s]\u001b[A\n",
            "  5%|▌         | 44472/874474 [00:02<00:52, 15679.19it/s]\u001b[A\n",
            "  5%|▌         | 46123/874474 [00:02<00:52, 15919.37it/s]\u001b[A\n",
            "  5%|▌         | 47793/874474 [00:02<00:51, 16145.12it/s]\u001b[A\n",
            "  6%|▌         | 49507/874474 [00:03<00:50, 16430.04it/s]\u001b[A\n",
            "  6%|▌         | 51214/874474 [00:03<00:49, 16615.57it/s]\u001b[A\n",
            "  6%|▌         | 52883/874474 [00:03<00:49, 16441.42it/s]\u001b[A\n",
            "  6%|▌         | 54533/874474 [00:03<00:50, 16263.47it/s]\u001b[A\n",
            "  6%|▋         | 56204/874474 [00:03<00:49, 16394.31it/s]\u001b[A\n",
            "  7%|▋         | 57867/874474 [00:03<00:49, 16463.77it/s]\u001b[A\n",
            "  7%|▋         | 59516/874474 [00:03<00:50, 16248.04it/s]\u001b[A\n",
            "  7%|▋         | 61190/874474 [00:03<00:49, 16390.33it/s]\u001b[A\n",
            "  7%|▋         | 62889/874474 [00:03<00:48, 16565.39it/s]\u001b[A\n",
            "  7%|▋         | 64641/874474 [00:04<00:48, 16840.66it/s]\u001b[A\n",
            "  8%|▊         | 66328/874474 [00:04<00:48, 16638.65it/s]\u001b[A\n",
            "  8%|▊         | 68013/874474 [00:04<00:48, 16699.59it/s]\u001b[A\n",
            "  8%|▊         | 69685/874474 [00:04<00:49, 16421.34it/s]\u001b[A\n",
            "  8%|▊         | 71368/874474 [00:04<00:48, 16541.50it/s]\u001b[A\n",
            "  8%|▊         | 73044/874474 [00:04<00:48, 16606.33it/s]\u001b[A\n",
            "  9%|▊         | 74816/874474 [00:04<00:47, 16925.19it/s]\u001b[A\n",
            "  9%|▊         | 76511/874474 [00:04<00:47, 16700.28it/s]\u001b[A\n",
            "  9%|▉         | 78214/874474 [00:04<00:47, 16797.50it/s]\u001b[A\n",
            "  9%|▉         | 79952/874474 [00:04<00:46, 16967.13it/s]\u001b[A\n",
            "  9%|▉         | 81696/874474 [00:05<00:46, 17104.48it/s]\u001b[A\n",
            " 10%|▉         | 83408/874474 [00:05<00:48, 16376.88it/s]\u001b[A\n",
            " 10%|▉         | 85120/874474 [00:05<00:47, 16591.68it/s]\u001b[A\n",
            " 10%|▉         | 86786/874474 [00:05<00:48, 16385.41it/s]\u001b[A\n",
            " 10%|█         | 88521/874474 [00:05<00:47, 16661.31it/s]\u001b[A\n",
            " 10%|█         | 90244/874474 [00:05<00:46, 16827.74it/s]\u001b[A\n",
            " 11%|█         | 91983/874474 [00:05<00:46, 16991.50it/s]\u001b[A\n",
            " 11%|█         | 93685/874474 [00:05<00:46, 16696.68it/s]\u001b[A\n",
            " 11%|█         | 95358/874474 [00:05<00:46, 16647.45it/s]\u001b[A\n",
            " 11%|█         | 97063/874474 [00:05<00:46, 16765.04it/s]\u001b[A\n",
            " 11%|█▏        | 98807/874474 [00:06<00:45, 16960.35it/s]\u001b[A\n",
            " 11%|█▏        | 100544/874474 [00:06<00:45, 17080.92it/s]\u001b[A\n",
            " 12%|█▏        | 102254/874474 [00:06<00:45, 17047.99it/s]\u001b[A\n",
            " 12%|█▏        | 103960/874474 [00:06<00:46, 16531.89it/s]\u001b[A\n",
            " 12%|█▏        | 105690/874474 [00:06<00:45, 16754.17it/s]\u001b[A\n",
            " 12%|█▏        | 107419/874474 [00:06<00:45, 16910.17it/s]\u001b[A\n",
            " 12%|█▏        | 109132/874474 [00:06<00:45, 16973.54it/s]\u001b[A\n",
            " 13%|█▎        | 110832/874474 [00:06<00:45, 16709.41it/s]\u001b[A\n",
            " 13%|█▎        | 112550/874474 [00:06<00:45, 16847.54it/s]\u001b[A\n",
            " 13%|█▎        | 114279/874474 [00:06<00:44, 16977.48it/s]\u001b[A\n",
            " 13%|█▎        | 116020/874474 [00:07<00:44, 17102.84it/s]\u001b[A\n",
            " 13%|█▎        | 117732/874474 [00:07<00:44, 17092.15it/s]\u001b[A\n",
            " 14%|█▎        | 119443/874474 [00:07<00:44, 17039.80it/s]\u001b[A\n",
            " 14%|█▍        | 121148/874474 [00:07<00:45, 16694.42it/s]\u001b[A\n",
            " 14%|█▍        | 122873/874474 [00:07<00:44, 16856.12it/s]\u001b[A\n",
            " 14%|█▍        | 124582/874474 [00:07<00:44, 16924.74it/s]\u001b[A\n",
            " 14%|█▍        | 126330/874474 [00:07<00:43, 17086.19it/s]\u001b[A\n",
            " 15%|█▍        | 128040/874474 [00:07<00:44, 16596.71it/s]\u001b[A\n",
            " 15%|█▍        | 129785/874474 [00:07<00:44, 16842.67it/s]\u001b[A\n",
            " 15%|█▌        | 131569/874474 [00:07<00:43, 17129.60it/s]\u001b[A\n",
            " 15%|█▌        | 133343/874474 [00:08<00:42, 17308.25it/s]\u001b[A\n",
            " 15%|█▌        | 135126/874474 [00:08<00:42, 17461.18it/s]\u001b[A\n",
            " 16%|█▌        | 136912/874474 [00:08<00:41, 17578.28it/s]\u001b[A\n",
            " 16%|█▌        | 138672/874474 [00:08<00:42, 17267.57it/s]\u001b[A\n",
            " 16%|█▌        | 140433/874474 [00:08<00:42, 17367.40it/s]\u001b[A\n",
            " 16%|█▋        | 142172/874474 [00:08<00:42, 17155.98it/s]\u001b[A\n",
            " 16%|█▋        | 143891/874474 [00:08<00:42, 17163.81it/s]\u001b[A\n",
            " 17%|█▋        | 145609/874474 [00:08<00:43, 16902.34it/s]\u001b[A\n",
            " 17%|█▋        | 147349/874474 [00:08<00:42, 17047.25it/s]\u001b[A\n",
            " 17%|█▋        | 149146/874474 [00:09<00:41, 17313.00it/s]\u001b[A\n",
            " 17%|█▋        | 150915/874474 [00:09<00:41, 17422.35it/s]\u001b[A\n",
            " 17%|█▋        | 152660/874474 [00:09<00:41, 17429.57it/s]\u001b[A\n",
            " 18%|█▊        | 154405/874474 [00:09<00:41, 17425.21it/s]\u001b[A\n",
            " 18%|█▊        | 156149/874474 [00:09<00:42, 16778.95it/s]\u001b[A\n",
            " 18%|█▊        | 157855/874474 [00:09<00:42, 16859.85it/s]\u001b[A\n",
            " 18%|█▊        | 159593/874474 [00:09<00:42, 17012.43it/s]\u001b[A\n",
            " 18%|█▊        | 161329/874474 [00:09<00:41, 17115.13it/s]\u001b[A\n",
            " 19%|█▊        | 163043/874474 [00:09<00:42, 16854.81it/s]\u001b[A\n",
            " 19%|█▉        | 164759/874474 [00:09<00:41, 16945.09it/s]\u001b[A\n",
            " 19%|█▉        | 166558/874474 [00:10<00:41, 17244.60it/s]\u001b[A\n",
            " 19%|█▉        | 168365/874474 [00:10<00:40, 17482.94it/s]\u001b[A\n",
            " 19%|█▉        | 170116/874474 [00:10<00:40, 17235.31it/s]\u001b[A\n",
            " 20%|█▉        | 171843/874474 [00:10<00:41, 17088.04it/s]\u001b[A\n",
            " 20%|█▉        | 173554/874474 [00:10<00:41, 16930.09it/s]\u001b[A\n",
            " 20%|██        | 175332/874474 [00:10<00:40, 17174.52it/s]\u001b[A\n",
            " 20%|██        | 177132/874474 [00:10<00:40, 17413.61it/s]\u001b[A\n",
            " 20%|██        | 178876/874474 [00:10<00:40, 17302.19it/s]\u001b[A\n",
            " 21%|██        | 180608/874474 [00:10<00:42, 16254.75it/s]\u001b[A\n",
            " 21%|██        | 182314/874474 [00:10<00:41, 16486.72it/s]\u001b[A\n",
            " 21%|██        | 184073/874474 [00:11<00:41, 16800.86it/s]\u001b[A\n",
            " 21%|██        | 185806/874474 [00:11<00:40, 16955.09it/s]\u001b[A\n",
            " 21%|██▏       | 187567/874474 [00:11<00:40, 17144.37it/s]\u001b[A\n",
            " 22%|██▏       | 189287/874474 [00:11<00:40, 17023.01it/s]\u001b[A\n",
            " 22%|██▏       | 190994/874474 [00:11<00:40, 16776.90it/s]\u001b[A\n",
            " 22%|██▏       | 192736/874474 [00:11<00:40, 16962.40it/s]\u001b[A\n",
            " 22%|██▏       | 194454/874474 [00:11<00:39, 17025.48it/s]\u001b[A\n",
            " 22%|██▏       | 196205/874474 [00:11<00:39, 17166.31it/s]\u001b[A\n",
            " 23%|██▎       | 197924/874474 [00:11<00:39, 16919.69it/s]\u001b[A\n",
            " 23%|██▎       | 199673/874474 [00:11<00:39, 17086.11it/s]\u001b[A\n",
            " 23%|██▎       | 201424/874474 [00:12<00:39, 17211.07it/s]\u001b[A\n",
            " 23%|██▎       | 203176/874474 [00:12<00:38, 17300.89it/s]\u001b[A\n",
            " 23%|██▎       | 204921/874474 [00:12<00:38, 17343.10it/s]\u001b[A\n",
            " 24%|██▎       | 206657/874474 [00:12<00:39, 16932.93it/s]\u001b[A\n",
            " 24%|██▍       | 208359/874474 [00:12<00:39, 16957.62it/s]\u001b[A\n",
            " 24%|██▍       | 210089/874474 [00:12<00:38, 17058.55it/s]\u001b[A\n",
            " 24%|██▍       | 211813/874474 [00:12<00:38, 17109.91it/s]\u001b[A\n",
            " 24%|██▍       | 213566/874474 [00:12<00:38, 17232.17it/s]\u001b[A\n",
            " 25%|██▍       | 215291/874474 [00:12<00:38, 16924.40it/s]\u001b[A\n",
            " 25%|██▍       | 217026/874474 [00:12<00:38, 17049.08it/s]\u001b[A\n",
            " 25%|██▌       | 218776/874474 [00:13<00:38, 17181.31it/s]\u001b[A\n",
            " 25%|██▌       | 220496/874474 [00:13<00:38, 16957.21it/s]\u001b[A\n",
            " 25%|██▌       | 222218/874474 [00:13<00:38, 17034.37it/s]\u001b[A\n",
            " 26%|██▌       | 223923/874474 [00:13<00:38, 16899.89it/s]\u001b[A\n",
            " 26%|██▌       | 225615/874474 [00:13<00:39, 16567.56it/s]\u001b[A\n",
            " 26%|██▌       | 227340/874474 [00:13<00:38, 16765.42it/s]\u001b[A\n",
            " 26%|██▌       | 229082/874474 [00:13<00:38, 16956.50it/s]\u001b[A\n",
            " 26%|██▋       | 230780/874474 [00:13<00:37, 16961.20it/s]\u001b[A\n",
            " 27%|██▋       | 232478/874474 [00:13<00:38, 16567.61it/s]\u001b[A\n",
            " 27%|██▋       | 234192/874474 [00:14<00:38, 16733.14it/s]\u001b[A\n",
            " 27%|██▋       | 235925/874474 [00:14<00:37, 16906.72it/s]\u001b[A\n",
            " 27%|██▋       | 237655/874474 [00:14<00:37, 17022.21it/s]\u001b[A\n",
            " 27%|██▋       | 239393/874474 [00:14<00:37, 17127.70it/s]\u001b[A\n",
            " 28%|██▊       | 241108/874474 [00:14<00:37, 16776.58it/s]\u001b[A\n",
            " 28%|██▊       | 242789/874474 [00:14<00:37, 16679.33it/s]\u001b[A\n",
            " 28%|██▊       | 244546/874474 [00:14<00:37, 16933.49it/s]\u001b[A\n",
            " 28%|██▊       | 246272/874474 [00:14<00:36, 17029.52it/s]\u001b[A\n",
            " 28%|██▊       | 248001/874474 [00:14<00:36, 17105.59it/s]\u001b[A\n",
            " 29%|██▊       | 249713/874474 [00:14<00:37, 16806.32it/s]\u001b[A\n",
            " 29%|██▉       | 251436/874474 [00:15<00:36, 16930.89it/s]\u001b[A\n",
            " 29%|██▉       | 253131/874474 [00:15<00:37, 16716.77it/s]\u001b[A\n",
            " 29%|██▉       | 254844/874474 [00:15<00:36, 16837.08it/s]\u001b[A\n",
            " 29%|██▉       | 256549/874474 [00:15<00:36, 16898.81it/s]\u001b[A\n",
            " 30%|██▉       | 258283/874474 [00:15<00:36, 17026.68it/s]\u001b[A\n",
            " 30%|██▉       | 259987/874474 [00:15<00:36, 16690.00it/s]\u001b[A\n",
            " 30%|██▉       | 261731/874474 [00:15<00:36, 16907.89it/s]\u001b[A\n",
            " 30%|███       | 263476/874474 [00:15<00:35, 17066.58it/s]\u001b[A\n",
            " 30%|███       | 265192/874474 [00:15<00:35, 17092.62it/s]\u001b[A\n",
            " 31%|███       | 266903/874474 [00:15<00:37, 16269.18it/s]\u001b[A\n",
            " 31%|███       | 268540/874474 [00:16<00:37, 16247.81it/s]\u001b[A\n",
            " 31%|███       | 270260/874474 [00:16<00:36, 16519.47it/s]\u001b[A\n",
            " 31%|███       | 271977/874474 [00:16<00:36, 16707.48it/s]\u001b[A\n",
            " 31%|███▏      | 273718/874474 [00:16<00:35, 16910.10it/s]\u001b[A\n",
            " 31%|███▏      | 275453/874474 [00:16<00:35, 17037.92it/s]\u001b[A\n",
            " 32%|███▏      | 277160/874474 [00:16<00:35, 16661.22it/s]\u001b[A\n",
            " 32%|███▏      | 278830/874474 [00:16<00:35, 16662.47it/s]\u001b[A\n",
            " 32%|███▏      | 280529/874474 [00:16<00:35, 16757.49it/s]\u001b[A\n",
            " 32%|███▏      | 282262/874474 [00:16<00:34, 16923.48it/s]\u001b[A\n",
            " 32%|███▏      | 283957/874474 [00:16<00:35, 16746.22it/s]\u001b[A\n",
            " 33%|███▎      | 285719/874474 [00:17<00:34, 16997.93it/s]\u001b[A\n",
            " 33%|███▎      | 287457/874474 [00:17<00:34, 17108.46it/s]\u001b[A\n",
            " 33%|███▎      | 289187/874474 [00:17<00:34, 17164.95it/s]\u001b[A\n",
            " 33%|███▎      | 290905/874474 [00:17<00:34, 17086.00it/s]\u001b[A\n",
            " 33%|███▎      | 292615/874474 [00:17<00:34, 16979.29it/s]\u001b[A\n",
            " 34%|███▎      | 294314/874474 [00:17<00:34, 16661.09it/s]\u001b[A\n",
            " 34%|███▍      | 296004/874474 [00:17<00:34, 16729.01it/s]\u001b[A\n",
            " 34%|███▍      | 297739/874474 [00:17<00:34, 16910.05it/s]\u001b[A\n",
            " 34%|███▍      | 299457/874474 [00:17<00:33, 16989.98it/s]\u001b[A\n",
            " 34%|███▍      | 301158/874474 [00:17<00:34, 16775.98it/s]\u001b[A\n",
            " 35%|███▍      | 302887/874474 [00:18<00:33, 16926.50it/s]\u001b[A\n",
            " 35%|███▍      | 304625/874474 [00:18<00:33, 17059.45it/s]\u001b[A\n",
            " 35%|███▌      | 306373/874474 [00:18<00:33, 17182.42it/s]\u001b[A\n",
            " 35%|███▌      | 308093/874474 [00:18<00:33, 17142.83it/s]\u001b[A\n",
            " 35%|███▌      | 309816/874474 [00:18<00:32, 17164.75it/s]\u001b[A\n",
            " 36%|███▌      | 311533/874474 [00:18<00:34, 16538.32it/s]\u001b[A\n",
            " 36%|███▌      | 313249/874474 [00:18<00:33, 16719.10it/s]\u001b[A\n",
            " 36%|███▌      | 314991/874474 [00:18<00:33, 16922.75it/s]\u001b[A\n",
            " 36%|███▌      | 316716/874474 [00:18<00:32, 17017.90it/s]\u001b[A\n",
            " 36%|███▋      | 318421/874474 [00:19<00:33, 16706.93it/s]\u001b[A\n",
            " 37%|███▋      | 320161/874474 [00:19<00:32, 16908.29it/s]\u001b[A\n",
            " 37%|███▋      | 321855/874474 [00:19<00:33, 16690.80it/s]\u001b[A\n",
            " 37%|███▋      | 323527/874474 [00:19<00:33, 16623.14it/s]\u001b[A\n",
            " 37%|███▋      | 325269/874474 [00:19<00:32, 16847.55it/s]\u001b[A\n",
            " 37%|███▋      | 326957/874474 [00:19<00:32, 16853.63it/s]\u001b[A\n",
            " 38%|███▊      | 328644/874474 [00:19<00:32, 16666.83it/s]\u001b[A\n",
            " 38%|███▊      | 330368/874474 [00:19<00:32, 16833.66it/s]\u001b[A\n",
            " 38%|███▊      | 332094/874474 [00:19<00:31, 16957.84it/s]\u001b[A\n",
            " 38%|███▊      | 333804/874474 [00:19<00:31, 16998.21it/s]\u001b[A\n",
            " 38%|███▊      | 335505/874474 [00:20<00:32, 16652.28it/s]\u001b[A\n",
            " 39%|███▊      | 337253/874474 [00:20<00:31, 16890.53it/s]\u001b[A\n",
            " 39%|███▉      | 339005/874474 [00:20<00:31, 17074.07it/s]\u001b[A\n",
            " 39%|███▉      | 340715/874474 [00:20<00:31, 17042.46it/s]\u001b[A\n",
            " 39%|███▉      | 342482/874474 [00:20<00:30, 17219.50it/s]\u001b[A\n",
            " 39%|███▉      | 344251/874474 [00:20<00:30, 17356.19it/s]\u001b[A\n",
            " 40%|███▉      | 345988/874474 [00:20<00:30, 17112.25it/s]\u001b[A\n",
            " 40%|███▉      | 347744/874474 [00:20<00:30, 17242.39it/s]\u001b[A\n",
            " 40%|███▉      | 349493/874474 [00:20<00:30, 17315.38it/s]\u001b[A\n",
            " 40%|████      | 351235/874474 [00:20<00:30, 17344.36it/s]\u001b[A\n",
            " 40%|████      | 352971/874474 [00:21<00:30, 16977.80it/s]\u001b[A\n",
            " 41%|████      | 354713/874474 [00:21<00:30, 17105.46it/s]\u001b[A\n",
            " 41%|████      | 356426/874474 [00:21<00:30, 17073.88it/s]\u001b[A\n",
            " 41%|████      | 358135/874474 [00:21<00:31, 16233.34it/s]\u001b[A\n",
            " 41%|████      | 359768/874474 [00:21<00:31, 16203.27it/s]\u001b[A\n",
            " 41%|████▏     | 361396/874474 [00:21<00:32, 16023.64it/s]\u001b[A\n",
            " 42%|████▏     | 363135/874474 [00:21<00:31, 16410.16it/s]\u001b[A\n",
            " 42%|████▏     | 364855/874474 [00:21<00:30, 16637.75it/s]\u001b[A\n",
            " 42%|████▏     | 366582/874474 [00:21<00:30, 16820.87it/s]\u001b[A\n",
            " 42%|████▏     | 368351/874474 [00:21<00:29, 17071.03it/s]\u001b[A\n",
            " 42%|████▏     | 370062/874474 [00:22<00:30, 16764.70it/s]\u001b[A\n",
            " 43%|████▎     | 371836/874474 [00:22<00:29, 17043.97it/s]\u001b[A\n",
            " 43%|████▎     | 373606/874474 [00:22<00:29, 17234.72it/s]\u001b[A\n",
            " 43%|████▎     | 375411/874474 [00:22<00:28, 17470.99it/s]\u001b[A\n",
            " 43%|████▎     | 377191/874474 [00:22<00:28, 17567.60it/s]\u001b[A\n",
            " 43%|████▎     | 378950/874474 [00:22<00:28, 17418.55it/s]\u001b[A\n",
            " 44%|████▎     | 380703/874474 [00:22<00:28, 17451.02it/s]\u001b[A\n",
            " 44%|████▎     | 382450/874474 [00:22<00:28, 17435.26it/s]\u001b[A\n",
            " 44%|████▍     | 384195/874474 [00:22<00:28, 16958.54it/s]\u001b[A\n",
            " 44%|████▍     | 385896/874474 [00:22<00:28, 16971.83it/s]\u001b[A\n",
            " 44%|████▍     | 387596/874474 [00:23<00:29, 16738.48it/s]\u001b[A\n",
            " 45%|████▍     | 389339/874474 [00:23<00:28, 16938.15it/s]\u001b[A\n",
            " 45%|████▍     | 391084/874474 [00:23<00:28, 17087.14it/s]\u001b[A\n",
            " 45%|████▍     | 392795/874474 [00:23<00:28, 16981.06it/s]\u001b[A\n",
            " 45%|████▌     | 394495/874474 [00:23<00:28, 16964.16it/s]\u001b[A\n",
            " 45%|████▌     | 396193/874474 [00:23<00:28, 16575.50it/s]\u001b[A\n",
            " 46%|████▌     | 397910/874474 [00:23<00:28, 16748.89it/s]\u001b[A\n",
            " 46%|████▌     | 399629/874474 [00:23<00:28, 16877.27it/s]\u001b[A\n",
            " 46%|████▌     | 401349/874474 [00:23<00:27, 16971.14it/s]\u001b[A\n",
            " 46%|████▌     | 403048/874474 [00:23<00:27, 16853.74it/s]\u001b[A\n",
            " 46%|████▋     | 404735/874474 [00:24<00:28, 16619.98it/s]\u001b[A\n",
            " 46%|████▋     | 406399/874474 [00:24<00:28, 16424.88it/s]\u001b[A\n",
            " 47%|████▋     | 408134/874474 [00:24<00:27, 16690.68it/s]\u001b[A\n",
            " 47%|████▋     | 409884/874474 [00:24<00:27, 16923.56it/s]\u001b[A\n",
            " 47%|████▋     | 411629/874474 [00:24<00:27, 17076.57it/s]\u001b[A\n",
            " 47%|████▋     | 413339/874474 [00:24<00:28, 16000.93it/s]\u001b[A\n",
            " 47%|████▋     | 415054/874474 [00:24<00:28, 16326.47it/s]\u001b[A\n",
            " 48%|████▊     | 416785/874474 [00:24<00:27, 16609.28it/s]\u001b[A\n",
            " 48%|████▊     | 418515/874474 [00:24<00:27, 16810.47it/s]\u001b[A\n",
            " 48%|████▊     | 420222/874474 [00:25<00:26, 16883.15it/s]\u001b[A\n",
            " 48%|████▊     | 421917/874474 [00:25<00:26, 16903.03it/s]\u001b[A\n",
            " 48%|████▊     | 423658/874474 [00:25<00:26, 17051.27it/s]\u001b[A\n",
            " 49%|████▊     | 425367/874474 [00:25<00:26, 16981.77it/s]\u001b[A\n",
            " 49%|████▉     | 427105/874474 [00:25<00:26, 17098.05it/s]\u001b[A\n",
            " 49%|████▉     | 428817/874474 [00:25<00:26, 17063.06it/s]\u001b[A\n",
            " 49%|████▉     | 430525/874474 [00:25<00:26, 16755.13it/s]\u001b[A\n",
            " 49%|████▉     | 432246/874474 [00:25<00:26, 16887.06it/s]\u001b[A\n",
            " 50%|████▉     | 433973/874474 [00:25<00:25, 16999.87it/s]\u001b[A\n",
            " 50%|████▉     | 435722/874474 [00:25<00:25, 17143.52it/s]\u001b[A\n",
            " 50%|█████     | 437472/874474 [00:26<00:25, 17248.15it/s]\u001b[A\n",
            " 50%|█████     | 439198/874474 [00:26<00:25, 16914.11it/s]\u001b[A\n",
            " 50%|█████     | 440892/874474 [00:26<00:25, 16874.06it/s]\u001b[A\n",
            " 51%|█████     | 442636/874474 [00:26<00:25, 17036.78it/s]\u001b[A\n",
            " 51%|█████     | 444342/874474 [00:26<00:25, 16842.16it/s]\u001b[A\n",
            " 51%|█████     | 446070/874474 [00:26<00:25, 16969.82it/s]\u001b[A\n",
            " 51%|█████     | 447769/874474 [00:26<00:25, 16615.03it/s]\u001b[A\n",
            " 51%|█████▏    | 449484/874474 [00:26<00:25, 16768.67it/s]\u001b[A\n",
            " 52%|█████▏    | 451221/874474 [00:26<00:24, 16944.05it/s]\u001b[A\n",
            " 52%|█████▏    | 452947/874474 [00:26<00:24, 17037.48it/s]\u001b[A\n",
            " 52%|█████▏    | 454705/874474 [00:27<00:24, 17195.86it/s]\u001b[A\n",
            " 52%|█████▏    | 456426/874474 [00:27<00:25, 16642.62it/s]\u001b[A\n",
            " 52%|█████▏    | 458160/874474 [00:27<00:24, 16844.10it/s]\u001b[A\n",
            " 53%|█████▎    | 459888/874474 [00:27<00:24, 16971.96it/s]\u001b[A\n",
            " 53%|█████▎    | 461636/874474 [00:27<00:24, 17120.76it/s]\u001b[A\n",
            " 53%|█████▎    | 463355/874474 [00:27<00:23, 17140.53it/s]\u001b[A\n",
            " 53%|█████▎    | 465071/874474 [00:27<00:24, 16856.92it/s]\u001b[A\n",
            " 53%|█████▎    | 466811/874474 [00:27<00:23, 17015.15it/s]\u001b[A\n",
            " 54%|█████▎    | 468547/874474 [00:27<00:23, 17116.26it/s]\u001b[A\n",
            " 54%|█████▍    | 470307/874474 [00:27<00:23, 17255.82it/s]\u001b[A\n",
            " 54%|█████▍    | 472119/874474 [00:28<00:22, 17504.03it/s]\u001b[A\n",
            " 54%|█████▍    | 473872/874474 [00:28<00:23, 17272.84it/s]\u001b[A\n",
            " 54%|█████▍    | 475618/874474 [00:28<00:23, 17327.56it/s]\u001b[A\n",
            " 55%|█████▍    | 477353/874474 [00:28<00:22, 17320.42it/s]\u001b[A\n",
            " 55%|█████▍    | 479163/874474 [00:28<00:22, 17545.38it/s]\u001b[A\n",
            " 55%|█████▍    | 480931/874474 [00:28<00:22, 17585.23it/s]\u001b[A\n",
            " 55%|█████▌    | 482691/874474 [00:28<00:22, 17213.08it/s]\u001b[A\n",
            " 55%|█████▌    | 484415/874474 [00:28<00:22, 17148.23it/s]\u001b[A\n",
            " 56%|█████▌    | 486132/874474 [00:28<00:22, 17119.53it/s]\u001b[A\n",
            " 56%|█████▌    | 487859/874474 [00:28<00:22, 17159.04it/s]\u001b[A\n",
            " 56%|█████▌    | 489609/874474 [00:29<00:22, 17258.49it/s]\u001b[A\n",
            " 56%|█████▌    | 491336/874474 [00:29<00:22, 16972.96it/s]\u001b[A\n",
            " 56%|█████▋    | 493065/874474 [00:29<00:22, 17064.87it/s]\u001b[A\n",
            " 57%|█████▋    | 494819/874474 [00:29<00:22, 17203.29it/s]\u001b[A\n",
            " 57%|█████▋    | 496588/874474 [00:29<00:21, 17344.85it/s]\u001b[A\n",
            " 57%|█████▋    | 498324/874474 [00:29<00:21, 17313.28it/s]\u001b[A\n",
            " 57%|█████▋    | 500057/874474 [00:29<00:22, 16533.28it/s]\u001b[A\n",
            " 57%|█████▋    | 501719/874474 [00:29<00:22, 16425.15it/s]\u001b[A\n",
            " 58%|█████▊    | 503495/874474 [00:29<00:22, 16799.95it/s]\u001b[A\n",
            " 58%|█████▊    | 505268/874474 [00:30<00:21, 17068.43it/s]\u001b[A\n",
            " 58%|█████▊    | 507079/874474 [00:30<00:21, 17365.97it/s]\u001b[A\n",
            " 58%|█████▊    | 508821/874474 [00:30<00:21, 17096.17it/s]\u001b[A\n",
            " 58%|█████▊    | 510593/874474 [00:30<00:21, 17276.70it/s]\u001b[A\n",
            " 59%|█████▊    | 512385/874474 [00:30<00:20, 17464.00it/s]\u001b[A\n",
            " 59%|█████▉    | 514135/874474 [00:30<00:21, 17109.51it/s]\u001b[A\n",
            " 59%|█████▉    | 515850/874474 [00:30<00:21, 16600.57it/s]\u001b[A\n",
            " 59%|█████▉    | 517516/874474 [00:30<00:21, 16457.94it/s]\u001b[A\n",
            " 59%|█████▉    | 519283/874474 [00:30<00:21, 16803.18it/s]\u001b[A\n",
            " 60%|█████▉    | 521044/874474 [00:30<00:20, 17036.59it/s]\u001b[A\n",
            " 60%|█████▉    | 522816/874474 [00:31<00:20, 17235.59it/s]\u001b[A\n",
            " 60%|█████▉    | 524544/874474 [00:31<00:20, 16978.63it/s]\u001b[A\n",
            " 60%|██████    | 526246/874474 [00:31<00:20, 16882.36it/s]\u001b[A\n",
            " 60%|██████    | 527986/874474 [00:31<00:20, 17032.15it/s]\u001b[A\n",
            " 61%|██████    | 529720/874474 [00:31<00:20, 17122.93it/s]\u001b[A\n",
            " 61%|██████    | 531456/874474 [00:31<00:19, 17191.42it/s]\u001b[A\n",
            " 61%|██████    | 533210/874474 [00:31<00:19, 17294.39it/s]\u001b[A\n",
            " 61%|██████    | 534941/874474 [00:31<00:20, 16893.59it/s]\u001b[A\n",
            " 61%|██████▏   | 536658/874474 [00:31<00:19, 16973.36it/s]\u001b[A\n",
            " 62%|██████▏   | 538358/874474 [00:31<00:20, 16754.37it/s]\u001b[A\n",
            " 62%|██████▏   | 540115/874474 [00:32<00:19, 16988.85it/s]\u001b[A\n",
            " 62%|██████▏   | 541847/874474 [00:32<00:19, 17083.70it/s]\u001b[A\n",
            " 62%|██████▏   | 543558/874474 [00:32<00:19, 16975.20it/s]\u001b[A\n",
            " 62%|██████▏   | 545257/874474 [00:32<00:19, 16822.89it/s]\u001b[A\n",
            " 63%|██████▎   | 546993/874474 [00:32<00:19, 16980.43it/s]\u001b[A\n",
            " 63%|██████▎   | 548734/874474 [00:32<00:19, 17106.87it/s]\u001b[A\n",
            " 63%|██████▎   | 550479/874474 [00:32<00:18, 17207.44it/s]\u001b[A\n",
            " 63%|██████▎   | 552201/874474 [00:32<00:19, 16807.10it/s]\u001b[A\n",
            " 63%|██████▎   | 553885/874474 [00:32<00:19, 16068.48it/s]\u001b[A\n",
            " 64%|██████▎   | 555605/874474 [00:32<00:19, 16390.99it/s]\u001b[A\n",
            " 64%|██████▎   | 557361/874474 [00:33<00:18, 16723.72it/s]\u001b[A\n",
            " 64%|██████▍   | 559057/874474 [00:33<00:18, 16793.33it/s]\u001b[A\n",
            " 64%|██████▍   | 560742/874474 [00:33<00:18, 16789.93it/s]\u001b[A\n",
            " 64%|██████▍   | 562513/874474 [00:33<00:18, 17052.36it/s]\u001b[A\n",
            " 65%|██████▍   | 564308/874474 [00:33<00:17, 17310.69it/s]\u001b[A\n",
            " 65%|██████▍   | 566086/874474 [00:33<00:17, 17447.26it/s]\u001b[A\n",
            " 65%|██████▍   | 567911/874474 [00:33<00:17, 17678.69it/s]\u001b[A\n",
            " 65%|██████▌   | 569682/874474 [00:33<00:17, 17337.68it/s]\u001b[A\n",
            " 65%|██████▌   | 571443/874474 [00:33<00:17, 17416.89it/s]\u001b[A\n",
            " 66%|██████▌   | 573188/874474 [00:34<00:18, 16560.16it/s]\u001b[A\n",
            " 66%|██████▌   | 574910/874474 [00:34<00:17, 16752.36it/s]\u001b[A\n",
            " 66%|██████▌   | 576594/874474 [00:34<00:17, 16590.78it/s]\u001b[A\n",
            " 66%|██████▌   | 578283/874474 [00:34<00:17, 16679.42it/s]\u001b[A\n",
            " 66%|██████▋   | 580050/874474 [00:34<00:17, 16963.53it/s]\u001b[A\n",
            " 67%|██████▋   | 581773/874474 [00:34<00:17, 17040.92it/s]\u001b[A\n",
            " 67%|██████▋   | 583539/874474 [00:34<00:16, 17221.64it/s]\u001b[A\n",
            " 67%|██████▋   | 585264/874474 [00:34<00:16, 17061.81it/s]\u001b[A\n",
            " 67%|██████▋   | 586973/874474 [00:34<00:17, 16465.65it/s]\u001b[A\n",
            " 67%|██████▋   | 588730/874474 [00:34<00:17, 16781.67it/s]\u001b[A\n",
            " 68%|██████▊   | 590457/874474 [00:35<00:16, 16924.31it/s]\u001b[A\n",
            " 68%|██████▊   | 592179/874474 [00:35<00:16, 17010.48it/s]\u001b[A\n",
            " 68%|██████▊   | 593884/874474 [00:35<00:16, 16565.73it/s]\u001b[A\n",
            " 68%|██████▊   | 595599/874474 [00:35<00:16, 16736.20it/s]\u001b[A\n",
            " 68%|██████▊   | 597345/874474 [00:35<00:16, 16946.91it/s]\u001b[A\n",
            " 69%|██████▊   | 599092/874474 [00:35<00:16, 17099.07it/s]\u001b[A\n",
            " 69%|██████▊   | 600805/874474 [00:35<00:15, 17106.95it/s]\u001b[A\n",
            " 69%|██████▉   | 602573/874474 [00:35<00:15, 17274.59it/s]\u001b[A\n",
            " 69%|██████▉   | 604303/874474 [00:35<00:16, 16665.24it/s]\u001b[A\n",
            " 69%|██████▉   | 606002/874474 [00:35<00:16, 16760.69it/s]\u001b[A\n",
            " 69%|██████▉   | 607737/874474 [00:36<00:15, 16932.93it/s]\u001b[A\n",
            " 70%|██████▉   | 609443/874474 [00:36<00:15, 16968.97it/s]\u001b[A\n",
            " 70%|██████▉   | 611143/874474 [00:36<00:15, 16601.00it/s]\u001b[A\n",
            " 70%|███████   | 612826/874474 [00:36<00:15, 16668.89it/s]\u001b[A\n",
            " 70%|███████   | 614517/874474 [00:36<00:15, 16738.79it/s]\u001b[A\n",
            " 70%|███████   | 616193/874474 [00:36<00:15, 16656.85it/s]\u001b[A\n",
            " 71%|███████   | 617922/874474 [00:36<00:15, 16841.13it/s]\u001b[A\n",
            " 71%|███████   | 619648/874474 [00:36<00:15, 16964.40it/s]\u001b[A\n",
            " 71%|███████   | 621346/874474 [00:36<00:15, 16689.57it/s]\u001b[A\n",
            " 71%|███████▏  | 623140/874474 [00:36<00:14, 17045.60it/s]\u001b[A\n",
            " 71%|███████▏  | 624848/874474 [00:37<00:14, 16999.79it/s]\u001b[A\n",
            " 72%|███████▏  | 626609/874474 [00:37<00:14, 17178.25it/s]\u001b[A\n",
            " 72%|███████▏  | 628329/874474 [00:37<00:14, 16828.62it/s]\u001b[A\n",
            " 72%|███████▏  | 630059/874474 [00:37<00:14, 16966.71it/s]\u001b[A\n",
            " 72%|███████▏  | 631806/874474 [00:37<00:14, 17112.86it/s]\u001b[A\n",
            " 72%|███████▏  | 633539/874474 [00:37<00:14, 17175.52it/s]\u001b[A\n",
            " 73%|███████▎  | 635302/874474 [00:37<00:13, 17307.72it/s]\u001b[A\n",
            " 73%|███████▎  | 637062/874474 [00:37<00:13, 17392.28it/s]\u001b[A\n",
            " 73%|███████▎  | 638803/874474 [00:37<00:14, 16774.79it/s]\u001b[A\n",
            " 73%|███████▎  | 640558/874474 [00:37<00:13, 16999.33it/s]\u001b[A\n",
            " 73%|███████▎  | 642313/874474 [00:38<00:13, 17159.54it/s]\u001b[A\n",
            " 74%|███████▎  | 644075/874474 [00:38<00:13, 17293.35it/s]\u001b[A\n",
            " 74%|███████▍  | 645808/874474 [00:38<00:13, 17139.28it/s]\u001b[A\n",
            " 74%|███████▍  | 647603/874474 [00:38<00:13, 17373.03it/s]\u001b[A\n",
            " 74%|███████▍  | 649400/874474 [00:38<00:12, 17547.81it/s]\u001b[A\n",
            " 74%|███████▍  | 651173/874474 [00:38<00:12, 17600.39it/s]\u001b[A\n",
            " 75%|███████▍  | 652980/874474 [00:38<00:12, 17736.93it/s]\u001b[A\n",
            " 75%|███████▍  | 654755/874474 [00:38<00:12, 17351.01it/s]\u001b[A\n",
            " 75%|███████▌  | 656493/874474 [00:38<00:12, 17080.25it/s]\u001b[A\n",
            " 75%|███████▌  | 658228/874474 [00:39<00:12, 17160.25it/s]\u001b[A\n",
            " 75%|███████▌  | 660011/874474 [00:39<00:12, 17355.07it/s]\u001b[A\n",
            " 76%|███████▌  | 661782/874474 [00:39<00:12, 17458.03it/s]\u001b[A\n",
            " 76%|███████▌  | 663530/874474 [00:39<00:12, 17123.30it/s]\u001b[A\n",
            " 76%|███████▌  | 665288/874474 [00:39<00:12, 17257.04it/s]\u001b[A\n",
            " 76%|███████▋  | 667016/874474 [00:39<00:12, 16474.12it/s]\u001b[A\n",
            " 76%|███████▋  | 668742/874474 [00:39<00:12, 16701.05it/s]\u001b[A\n",
            " 77%|███████▋  | 670484/874474 [00:39<00:12, 16908.17it/s]\u001b[A\n",
            " 77%|███████▋  | 672193/874474 [00:39<00:11, 16960.32it/s]\u001b[A\n",
            " 77%|███████▋  | 673911/874474 [00:39<00:11, 17024.70it/s]\u001b[A\n",
            " 77%|███████▋  | 675672/874474 [00:40<00:11, 17194.56it/s]\u001b[A\n",
            " 77%|███████▋  | 677455/874474 [00:40<00:11, 17379.17it/s]\u001b[A\n",
            " 78%|███████▊  | 679196/874474 [00:40<00:11, 17342.34it/s]\u001b[A\n",
            " 78%|███████▊  | 680932/874474 [00:40<00:11, 16981.74it/s]\u001b[A\n",
            " 78%|███████▊  | 682683/874474 [00:40<00:11, 17134.88it/s]\u001b[A\n",
            " 78%|███████▊  | 684399/874474 [00:40<00:11, 17063.45it/s]\u001b[A\n",
            " 78%|███████▊  | 686107/874474 [00:40<00:11, 16819.49it/s]\u001b[A\n",
            " 79%|███████▊  | 687791/874474 [00:40<00:11, 16774.66it/s]\u001b[A\n",
            " 79%|███████▉  | 689506/874474 [00:40<00:10, 16882.50it/s]\u001b[A\n",
            " 79%|███████▉  | 691196/874474 [00:40<00:10, 16771.60it/s]\u001b[A\n",
            " 79%|███████▉  | 692941/874474 [00:41<00:10, 16967.88it/s]\u001b[A\n",
            " 79%|███████▉  | 694707/874474 [00:41<00:10, 17168.39it/s]\u001b[A\n",
            " 80%|███████▉  | 696450/874474 [00:41<00:10, 17245.05it/s]\u001b[A\n",
            " 80%|███████▉  | 698176/874474 [00:41<00:10, 16749.25it/s]\u001b[A\n",
            " 80%|████████  | 699901/874474 [00:41<00:10, 16896.31it/s]\u001b[A\n",
            " 80%|████████  | 701611/874474 [00:41<00:10, 16956.31it/s]\u001b[A\n",
            " 80%|████████  | 703370/874474 [00:41<00:09, 17139.89it/s]\u001b[A\n",
            " 81%|████████  | 705105/874474 [00:41<00:09, 17202.24it/s]\u001b[A\n",
            " 81%|████████  | 706868/874474 [00:41<00:09, 17323.31it/s]\u001b[A\n",
            " 81%|████████  | 708602/874474 [00:41<00:09, 16895.93it/s]\u001b[A\n",
            " 81%|████████  | 710295/874474 [00:42<00:09, 16863.81it/s]\u001b[A\n",
            " 81%|████████▏ | 712035/874474 [00:42<00:09, 17020.36it/s]\u001b[A\n",
            " 82%|████████▏ | 713775/874474 [00:42<00:09, 17130.60it/s]\u001b[A\n",
            " 82%|████████▏ | 715490/874474 [00:42<00:09, 16640.66it/s]\u001b[A\n",
            " 82%|████████▏ | 717238/874474 [00:42<00:09, 16882.55it/s]\u001b[A\n",
            " 82%|████████▏ | 718958/874474 [00:42<00:09, 16975.02it/s]\u001b[A\n",
            " 82%|████████▏ | 720659/874474 [00:42<00:09, 16980.59it/s]\u001b[A\n",
            " 83%|████████▎ | 722409/874474 [00:42<00:08, 17133.09it/s]\u001b[A\n",
            " 83%|████████▎ | 724140/874474 [00:42<00:08, 17183.89it/s]\u001b[A\n",
            " 83%|████████▎ | 725860/874474 [00:42<00:08, 16942.29it/s]\u001b[A\n",
            " 83%|████████▎ | 727556/874474 [00:43<00:08, 16944.07it/s]\u001b[A\n",
            " 83%|████████▎ | 729252/874474 [00:43<00:08, 16398.17it/s]\u001b[A\n",
            " 84%|████████▎ | 731004/874474 [00:43<00:08, 16718.45it/s]\u001b[A\n",
            " 84%|████████▍ | 732681/874474 [00:43<00:08, 16538.24it/s]\u001b[A\n",
            " 84%|████████▍ | 734438/874474 [00:43<00:08, 16834.11it/s]\u001b[A\n",
            " 84%|████████▍ | 736193/874474 [00:43<00:08, 17041.94it/s]\u001b[A\n",
            " 84%|████████▍ | 737901/874474 [00:43<00:08, 16970.98it/s]\u001b[A\n",
            " 85%|████████▍ | 739674/874474 [00:43<00:07, 17190.20it/s]\u001b[A\n",
            " 85%|████████▍ | 741422/874474 [00:43<00:07, 17274.36it/s]\u001b[A\n",
            " 85%|████████▍ | 743152/874474 [00:44<00:07, 16943.29it/s]\u001b[A\n",
            " 85%|████████▌ | 744858/874474 [00:44<00:07, 16975.61it/s]\u001b[A\n",
            " 85%|████████▌ | 746613/874474 [00:44<00:07, 17143.26it/s]\u001b[A\n",
            " 86%|████████▌ | 748388/874474 [00:44<00:07, 17318.90it/s]\u001b[A\n",
            " 86%|████████▌ | 750122/874474 [00:44<00:07, 16620.35it/s]\u001b[A\n",
            " 86%|████████▌ | 751830/874474 [00:44<00:07, 16754.14it/s]\u001b[A\n",
            " 86%|████████▌ | 753565/874474 [00:44<00:07, 16926.09it/s]\u001b[A\n",
            " 86%|████████▋ | 755303/874474 [00:44<00:06, 17058.78it/s]\u001b[A\n",
            " 87%|████████▋ | 757059/874474 [00:44<00:06, 17206.06it/s]\u001b[A\n",
            " 87%|████████▋ | 758783/874474 [00:44<00:06, 17099.82it/s]\u001b[A\n",
            " 87%|████████▋ | 760495/874474 [00:45<00:06, 16761.22it/s]\u001b[A\n",
            " 87%|████████▋ | 762253/874474 [00:45<00:06, 16998.34it/s]\u001b[A\n",
            " 87%|████████▋ | 764011/874474 [00:45<00:06, 17166.66it/s]\u001b[A\n",
            " 88%|████████▊ | 765753/874474 [00:45<00:06, 17241.39it/s]\u001b[A\n",
            " 88%|████████▊ | 767479/874474 [00:45<00:06, 16927.16it/s]\u001b[A\n",
            " 88%|████████▊ | 769193/874474 [00:45<00:06, 16989.52it/s]\u001b[A\n",
            " 88%|████████▊ | 770942/874474 [00:45<00:06, 17133.77it/s]\u001b[A\n",
            " 88%|████████▊ | 772664/874474 [00:45<00:05, 17156.21it/s]\u001b[A\n",
            " 89%|████████▊ | 774381/874474 [00:45<00:05, 17152.86it/s]\u001b[A\n",
            " 89%|████████▉ | 776098/874474 [00:45<00:05, 16946.62it/s]\u001b[A\n",
            " 89%|████████▉ | 777794/874474 [00:46<00:05, 16839.36it/s]\u001b[A\n",
            " 89%|████████▉ | 779541/874474 [00:46<00:05, 17023.27it/s]\u001b[A\n",
            " 89%|████████▉ | 781308/874474 [00:46<00:05, 17209.79it/s]\u001b[A\n",
            " 90%|████████▉ | 783052/874474 [00:46<00:05, 17275.77it/s]\u001b[A\n",
            " 90%|████████▉ | 784781/874474 [00:46<00:05, 16895.06it/s]\u001b[A\n",
            " 90%|████████▉ | 786474/874474 [00:46<00:05, 16741.86it/s]\u001b[A\n",
            " 90%|█████████ | 788208/874474 [00:46<00:05, 16914.64it/s]\u001b[A\n",
            " 90%|█████████ | 789938/874474 [00:46<00:04, 17027.31it/s]\u001b[A\n",
            " 91%|█████████ | 791683/874474 [00:46<00:04, 17150.77it/s]\u001b[A\n",
            " 91%|█████████ | 793400/874474 [00:46<00:04, 17052.86it/s]\u001b[A\n",
            " 91%|█████████ | 795107/874474 [00:47<00:04, 16947.82it/s]\u001b[A\n",
            " 91%|█████████ | 796854/874474 [00:47<00:04, 17099.95it/s]\u001b[A\n",
            " 91%|█████████▏| 798592/874474 [00:47<00:04, 17181.45it/s]\u001b[A\n",
            " 92%|█████████▏| 800342/874474 [00:47<00:04, 17275.03it/s]\u001b[A\n",
            " 92%|█████████▏| 802071/874474 [00:47<00:04, 16917.51it/s]\u001b[A\n",
            " 92%|█████████▏| 803765/874474 [00:47<00:04, 16752.37it/s]\u001b[A\n",
            " 92%|█████████▏| 805485/874474 [00:47<00:04, 16881.40it/s]\u001b[A\n",
            " 92%|█████████▏| 807221/874474 [00:47<00:03, 17020.72it/s]\u001b[A\n",
            " 93%|█████████▎| 808974/874474 [00:47<00:03, 17169.85it/s]\u001b[A\n",
            " 93%|█████████▎| 810725/874474 [00:47<00:03, 17268.32it/s]\u001b[A\n",
            " 93%|█████████▎| 812453/874474 [00:48<00:03, 16997.62it/s]\u001b[A\n",
            " 93%|█████████▎| 814181/874474 [00:48<00:03, 17079.42it/s]\u001b[A\n",
            " 93%|█████████▎| 815891/874474 [00:48<00:03, 16874.83it/s]\u001b[A\n",
            " 93%|█████████▎| 817580/874474 [00:48<00:03, 16390.10it/s]\u001b[A\n",
            " 94%|█████████▎| 819224/874474 [00:48<00:03, 16326.12it/s]\u001b[A\n",
            " 94%|█████████▍| 820966/874474 [00:48<00:03, 16635.89it/s]\u001b[A\n",
            " 94%|█████████▍| 822711/874474 [00:48<00:03, 16869.87it/s]\u001b[A\n",
            " 94%|█████████▍| 824454/874474 [00:48<00:02, 17033.29it/s]\u001b[A\n",
            " 94%|█████████▍| 826182/874474 [00:48<00:02, 17104.37it/s]\u001b[A\n",
            " 95%|█████████▍| 827895/874474 [00:48<00:02, 16583.64it/s]\u001b[A\n",
            " 95%|█████████▍| 829574/874474 [00:49<00:02, 16642.39it/s]\u001b[A\n",
            " 95%|█████████▌| 831331/874474 [00:49<00:02, 16908.28it/s]\u001b[A\n",
            " 95%|█████████▌| 833066/874474 [00:49<00:02, 17035.59it/s]\u001b[A\n",
            " 95%|█████████▌| 834811/874474 [00:49<00:02, 17155.94it/s]\u001b[A\n",
            " 96%|█████████▌| 836529/874474 [00:49<00:02, 16884.55it/s]\u001b[A\n",
            " 96%|█████████▌| 838278/874474 [00:49<00:02, 17060.08it/s]\u001b[A\n",
            " 96%|█████████▌| 840027/874474 [00:49<00:02, 17183.35it/s]\u001b[A\n",
            " 96%|█████████▋| 841774/874474 [00:49<00:01, 17267.28it/s]\u001b[A\n",
            " 96%|█████████▋| 843503/874474 [00:49<00:01, 16969.13it/s]\u001b[A\n",
            " 97%|█████████▋| 845202/874474 [00:50<00:01, 16703.16it/s]\u001b[A\n",
            " 97%|█████████▋| 846963/874474 [00:50<00:01, 16963.62it/s]\u001b[A\n",
            " 97%|█████████▋| 848720/874474 [00:50<00:01, 17140.66it/s]\u001b[A\n",
            " 97%|█████████▋| 850472/874474 [00:50<00:01, 17252.58it/s]\u001b[A\n",
            " 97%|█████████▋| 852259/874474 [00:50<00:01, 17431.45it/s]\u001b[A\n",
            " 98%|█████████▊| 854004/874474 [00:50<00:01, 16977.47it/s]\u001b[A\n",
            " 98%|█████████▊| 855706/874474 [00:50<00:01, 16962.97it/s]\u001b[A\n",
            " 98%|█████████▊| 857453/874474 [00:50<00:00, 17108.92it/s]\u001b[A\n",
            " 98%|█████████▊| 859167/874474 [00:50<00:00, 16883.44it/s]\u001b[A\n",
            " 98%|█████████▊| 860907/874474 [00:50<00:00, 17031.22it/s]\u001b[A\n",
            " 99%|█████████▊| 862612/874474 [00:51<00:00, 16794.21it/s]\u001b[A\n",
            " 99%|█████████▉| 864363/874474 [00:51<00:00, 17001.15it/s]\u001b[A\n",
            " 99%|█████████▉| 866146/874474 [00:51<00:00, 17236.35it/s]\u001b[A\n",
            " 99%|█████████▉| 867916/874474 [00:51<00:00, 17371.00it/s]\u001b[A\n",
            " 99%|█████████▉| 869701/874474 [00:51<00:00, 17509.42it/s]\u001b[A\n",
            "100%|█████████▉| 871454/874474 [00:51<00:00, 17177.88it/s]\u001b[A\n",
            "100%|█████████▉| 873205/874474 [00:51<00:00, 17274.04it/s]\u001b[A"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PqRap7oC-1F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "outputId": "8574bb48-308d-4f8b-bca0-1f11db316f6e"
      },
      "source": [
        "while 1:\n",
        "  continue"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-de7f810681bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0aXkbnHHDdd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a,b,c = dic['iters']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQwZu28UUEWd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab,_,_ = dic['vocabs']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZLdZPK7UPXA",
        "colab_type": "code",
        "outputId": "74a8b59c-3b34-463c-dae4-5c5717a15197",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "vsz = len(vocab)\n",
        "print(vsz)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "26874\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbTwNX2TPFUq",
        "colab_type": "code",
        "outputId": "195211bb-22a1-4d6e-d916-d378577a7d7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        }
      },
      "source": [
        "t\n",
        "for t in enumerate(a):\n",
        "  print(t)\n",
        "  print(len(t))\n",
        "  t=t[1]\n",
        "  print(t[1])\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(0, \n",
            "[torchtext.data.batch.Batch of size 1]\n",
            "\t[.labels]:[torch.LongTensor of size 1x50]\n",
            "\t[.inputs_word]:[torch.LongTensor of size 1x100]\n",
            "\t[.inputs_char]:[torch.LongTensor of size 1x9x12])\n",
            "2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-a23a513da4b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'Batch' object does not support indexing"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpVAP3RgPGlp",
        "colab_type": "code",
        "outputId": "8f050577-9ee0-4ef4-d886-aca165f1317a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "t"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "[torchtext.data.batch.Batch of size 1]\n",
              "\t[.labels]:[torch.LongTensor of size 1x50]\n",
              "\t[.inputs_word]:[torch.LongTensor of size 1x100]\n",
              "\t[.inputs_char]:[torch.LongTensor of size 1x9x12]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XHNN88EQooN",
        "colab_type": "code",
        "outputId": "327cce9b-caad-44b6-bf48-8a669764a428",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "t.inputs_word.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 100])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5H9FyaZQw01",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class LSTM_Tagger(nn.Module):\n",
        "    def __init__(self, params):\n",
        "        super(LSTM_Tagger, self).__init__()\n",
        "\n",
        "        #maps each token to an embedding_dim vector\n",
        "        self.embedding = nn.Embedding(params['vocab_size'], params['embedding_dim'])\n",
        "\n",
        "        #the LSTM takens embedded sentence\n",
        "        self.lstm = nn.LSTM(params['embedding_dim'], params['lstm_hidden_dim'], batch_first=True)\n",
        "\n",
        "        #fc layer transforms the output to give the final output layer\n",
        "        self.fc = nn.Linear(params['lstm_hidden_dim'], params['num_tags'])\n",
        "\n",
        "    def forward(self, s):\n",
        "        #apply the embedding layer that maps each token to its embedding\n",
        "        s = self.embedding(s)   # dim: batch_size x batch_max_len x embedding_dim\n",
        "\n",
        "        #run the LSTM along the sentences of length batch_max_len\n",
        "        s, _ = self.lstm(s)     # dim: batch_size x batch_max_len x lstm_hidden_dim                \n",
        "\n",
        "        print(s.shape)\n",
        "        #reshape the Variable so that each row contains one token\n",
        "        #s = s.reshape(-1, s.shape[2])  # dim: batch_size*batch_max_len x lstm_hidden_dim\n",
        "\n",
        "        #apply the fully connected layer and obtain the output for each token\n",
        "        s = self.fc(s)          # dim: batch_size*batch_max_len x num_tags\n",
        "\n",
        "        return F.log_softmax(s, dim=2)   # dim: batch_size*batch_max_len x num_tags\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "le-wpgUeSwo7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding = nn.Embedding(vsz, 300)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grom9WkTUljy",
        "colab_type": "code",
        "outputId": "d4bc5f73-9830-4454-e402-be44ac40a2fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(embedding(t.inputs_word).shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 34, 300])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFytRdKrUwHO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cpu\")\n",
        "def loss_fn(outputs, labels):\n",
        "  print(labels, labels.shape)\n",
        "  print(outputs,outputs.shape)\n",
        "  scores = nn.functional.cross_entropy(outputs, labels)\n",
        "  return scores\n",
        "\n",
        "def clip_gradient(model, clip_value):\n",
        "    params = list(filter(lambda p: p.grad is not None, model.parameters()))\n",
        "    for p in params:\n",
        "        p.grad.data.clamp_(-clip_value, clip_value)\n",
        "    \n",
        "def train_model(model, train_iter, epoch):\n",
        "    total_epoch_loss = 0\n",
        "    total_epoch_acc = 0\n",
        "    model.to(device)\n",
        "    \n",
        "    optim = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()))\n",
        "    steps = 0\n",
        "    model.train()\n",
        "    for idx, batch in enumerate(train_iter):\n",
        "        text = batch.inputs_word\n",
        "       \n",
        "        target = batch.labels\n",
        "        target = torch.autograd.Variable(target).long()\n",
        "        if torch.cuda.is_available():\n",
        "            text = text.to(device)\n",
        "            target = target.to(device)\n",
        "        if (text.size()[0] is not 64):# One of the batch returned by BucketIterator has length different than 32.\n",
        "            continue\n",
        "        optim.zero_grad()\n",
        "        prediction = model(text)\n",
        "        loss = loss_fn(prediction, target)\n",
        "        num_corrects = (torch.max(prediction, 1)[1].view(target.size()).data == target.data).float().sum()\n",
        "        acc = 100.0 * num_corrects/len(batch)\n",
        "        loss.backward()\n",
        "        clip_gradient(model, 1e-1)\n",
        "        optim.step()\n",
        "        steps += 1\n",
        "        \n",
        "        if steps % 100 == 0:\n",
        "            print (f'Epoch: {epoch+1}, Idx: {idx+1}, Training Loss: {loss.item():.4f}, Training Accuracy: {acc.item(): .2f}%')\n",
        "        \n",
        "        total_epoch_loss += loss.item()\n",
        "        total_epoch_acc += acc.item()\n",
        "        \n",
        "    return total_epoch_loss/len(train_iter), total_epoch_acc/len(train_iter)\n",
        "\n",
        "def eval_model(model, val_iter):\n",
        "    total_epoch_loss = 0\n",
        "    total_epoch_acc = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for idx, batch in enumerate(val_iter):\n",
        "            text = batch.inputs_word\n",
        "            if (text.size()[0] is not 64):\n",
        "                continue\n",
        "            target = batch.labels\n",
        "            target = torch.autograd.Variable(target).long()\n",
        "            if torch.cuda.is_available():\n",
        "                text = text.cuda()\n",
        "                target = target.cuda()\n",
        "            prediction = model(text)\n",
        "            loss = loss_fn(prediction, target)\n",
        "            num_corrects = (torch.max(prediction, 1)[1].view(target.size()).data == target.data).sum()\n",
        "            acc = 100.0 * num_corrects/len(batch)\n",
        "            total_epoch_loss += loss.item()\n",
        "            total_epoch_acc += acc.item()\n",
        "\n",
        "    return total_epoch_loss/len(val_iter), total_epoch_acc/len(val_iter)\n",
        "\t\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USEclqAO6Xo4",
        "colab_type": "code",
        "outputId": "a3370ca3-65f8-415c-a953-95c5f111bb48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "learning_rate = 2e-5\n",
        "batch_size = 64\n",
        "\n",
        "lstm_hidden_dim = 256\n",
        "num_tags = 17\n",
        "embedding_length = 300\n",
        "train_iter,valid_iter,test_iter = dic['iters']\n",
        "vocab,_,_ = dic['vocabs']\n",
        "vsz = len(vocab)\n",
        "params={'vocab_size':vsz, 'embedding_dim':embedding_length,'lstm_hidden_dim': lstm_hidden_dim, 'num_tags':num_tags}\n",
        "model = LSTM_Tagger(params)\n",
        "for epoch in range(10):\n",
        "    train_loss, train_acc = train_model(model, train_iter, epoch)\n",
        "    val_loss, val_acc = eval_model(model, valid_iter)\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.2f}%, Val. Loss: {val_loss:3f}, Val. Acc: {val_acc:.2f}%')\n",
        "    \n",
        "test_loss, test_acc = eval_model(model, test_iter)\n",
        "print(f'Test Loss: {test_loss:.3f}, Test Acc: {test_acc:.2f}%')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([64, 100, 256])\n",
            "tensor([[2, 5, 5,  ..., 1, 1, 1],\n",
            "        [2, 5, 5,  ..., 1, 1, 1],\n",
            "        [2, 4, 4,  ..., 1, 1, 1],\n",
            "        ...,\n",
            "        [2, 5, 4,  ..., 1, 1, 1],\n",
            "        [2, 4, 4,  ..., 1, 1, 1],\n",
            "        [2, 4, 3,  ..., 1, 1, 1]]) torch.Size([64, 50])\n",
            "tensor([[[-2.7340, -2.8007, -2.8329,  ..., -2.8430, -2.8111, -2.8385],\n",
            "         [-2.6978, -2.7989, -2.8566,  ..., -2.8514, -2.7225, -2.7994],\n",
            "         [-2.8602, -2.8705, -2.8598,  ..., -2.8194, -2.8263, -2.8451],\n",
            "         ...,\n",
            "         [-2.8433, -2.6095, -2.7790,  ..., -2.6710, -2.9373, -2.7856],\n",
            "         [-2.8433, -2.6095, -2.7790,  ..., -2.6710, -2.9373, -2.7856],\n",
            "         [-2.8433, -2.6095, -2.7790,  ..., -2.6710, -2.9373, -2.7856]],\n",
            "\n",
            "        [[-2.7340, -2.8007, -2.8329,  ..., -2.8430, -2.8111, -2.8385],\n",
            "         [-2.8009, -2.9264, -2.8944,  ..., -2.8877, -2.9326, -2.8886],\n",
            "         [-2.7585, -2.7971, -2.9375,  ..., -2.9128, -2.8832, -2.9002],\n",
            "         ...,\n",
            "         [-2.8433, -2.6095, -2.7790,  ..., -2.6710, -2.9373, -2.7856],\n",
            "         [-2.8433, -2.6095, -2.7790,  ..., -2.6710, -2.9373, -2.7856],\n",
            "         [-2.8433, -2.6095, -2.7790,  ..., -2.6710, -2.9373, -2.7856]],\n",
            "\n",
            "        [[-2.7340, -2.8007, -2.8329,  ..., -2.8430, -2.8111, -2.8385],\n",
            "         [-2.7072, -2.8228, -2.8764,  ..., -2.8346, -2.8024, -2.7461],\n",
            "         [-2.8966, -2.8019, -2.8519,  ..., -2.7455, -2.8225, -2.8185],\n",
            "         ...,\n",
            "         [-2.8433, -2.6095, -2.7790,  ..., -2.6710, -2.9373, -2.7856],\n",
            "         [-2.8433, -2.6095, -2.7790,  ..., -2.6710, -2.9373, -2.7856],\n",
            "         [-2.8433, -2.6095, -2.7790,  ..., -2.6710, -2.9373, -2.7856]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-2.7340, -2.8007, -2.8329,  ..., -2.8430, -2.8111, -2.8385],\n",
            "         [-2.8035, -2.8088, -2.8908,  ..., -2.7910, -2.7612, -2.8360],\n",
            "         [-2.8285, -2.9007, -2.7272,  ..., -2.7175, -2.7988, -2.8567],\n",
            "         ...,\n",
            "         [-2.8433, -2.6095, -2.7790,  ..., -2.6710, -2.9373, -2.7856],\n",
            "         [-2.8433, -2.6095, -2.7790,  ..., -2.6710, -2.9373, -2.7856],\n",
            "         [-2.8433, -2.6095, -2.7790,  ..., -2.6710, -2.9373, -2.7856]],\n",
            "\n",
            "        [[-2.7340, -2.8007, -2.8329,  ..., -2.8430, -2.8111, -2.8385],\n",
            "         [-2.8968, -2.7242, -2.8335,  ..., -2.8222, -2.8101, -2.8809],\n",
            "         [-2.8193, -2.8090, -2.9204,  ..., -2.8160, -2.8167, -2.8423],\n",
            "         ...,\n",
            "         [-2.8433, -2.6095, -2.7790,  ..., -2.6710, -2.9373, -2.7856],\n",
            "         [-2.8433, -2.6095, -2.7790,  ..., -2.6710, -2.9373, -2.7856],\n",
            "         [-2.8433, -2.6095, -2.7790,  ..., -2.6710, -2.9373, -2.7856]],\n",
            "\n",
            "        [[-2.7340, -2.8007, -2.8329,  ..., -2.8430, -2.8111, -2.8385],\n",
            "         [-2.8529, -2.8653, -2.9507,  ..., -2.8488, -2.7464, -2.6921],\n",
            "         [-2.8912, -2.8674, -2.7778,  ..., -2.8133, -2.8587, -2.7751],\n",
            "         ...,\n",
            "         [-2.8433, -2.6095, -2.7790,  ..., -2.6710, -2.9373, -2.7856],\n",
            "         [-2.8433, -2.6095, -2.7790,  ..., -2.6710, -2.9373, -2.7856],\n",
            "         [-2.8433, -2.6095, -2.7790,  ..., -2.6710, -2.9373, -2.7856]]],\n",
            "       grad_fn=<LogSoftmaxBackward>) torch.Size([64, 100, 17])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-97783bb811bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTM_Tagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-2123703bc184>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_iter, epoch)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mnum_corrects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100.0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_corrects\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-2123703bc184>\u001b[0m in \u001b[0;36mloss_fn\u001b[0;34m(outputs, labels)\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2019\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2020\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2021\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2023\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   1846\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m             raise ValueError('Expected target size {}, got {}'.format(\n\u001b[0;32m-> 1848\u001b[0;31m                 out_size, target.size()))\n\u001b[0m\u001b[1;32m   1849\u001b[0m         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Expected target size (64, 17), got torch.Size([64, 50])"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrW7tPOc7jLg",
        "colab_type": "code",
        "outputId": "ca2a1f0b-b922-40dc-896e-938e8d4805a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "vsz = len(dic['vocabs'])\n",
        "vsz"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78I3kUZd7oiJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}