{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lSTM_colab.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN+HeV5gfpYO9SJiXzJAx3I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nilanshrajput/NER_SyferText/blob/master/lSTM_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBGOlkdKCP9O",
        "colab_type": "code",
        "outputId": "50811616-45e3-4727-aeef-77a11038902e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "!git clone https://github.com/synalp/NER.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'NER'...\n",
            "remote: Enumerating objects: 3148, done.\u001b[K\n",
            "remote: Total 3148 (delta 0), reused 0 (delta 0), pack-reused 3148\u001b[K\n",
            "Receiving objects: 100% (3148/3148), 281.51 MiB | 29.43 MiB/s, done.\n",
            "Resolving deltas: 100% (2066/2066), done.\n",
            "Checking out files: 100% (189/189), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viGkubaPGPWl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from __future__ import print_function\n",
        "from collections import OrderedDict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import init\n",
        "from torch.autograd import Variable\n",
        "from torch import autograd\n",
        "\n",
        "import time\n",
        "import _pickle as cPickle\n",
        "\n",
        "import urllib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import codecs\n",
        "import re\n",
        "import numpy as np\n",
        "import spacy\n",
        "from sklearn.metrics import f1_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PqRap7oC-1F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "outputId": "e48a6976-7bc8-4de4-838b-92100f74847a"
      },
      "source": [
        "\"\"\"while 1:\n",
        "  continue\"\"\""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-de7f810681bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lW7VtQ2yCiPL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import torch\n",
        "from torchtext import data\n",
        "from torchtext.datasets import SequenceTaggingDataset\n",
        "from torchtext.vocab import Vectors, GloVe\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import logging\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "def conll2003_dataset(tag_type, batch_size, root='/content/NER/corpus/CoNLL-2003', \n",
        "                          train_file='eng.train', \n",
        "                          validation_file='eng.testa',\n",
        "                          test_file='eng.testb',\n",
        "                          convert_digits=True):\n",
        "    \"\"\"\n",
        "    conll2003: Conll 2003 (Parser only. You must place the files)\n",
        "    Extract Conll2003 dataset using torchtext. Applies GloVe 6B.200d and Char N-gram\n",
        "    pretrained vectors. Also sets up per word character Field\n",
        "    Parameters:\n",
        "        tag_type: Type of tag to pick as task [pos, chunk, ner]\n",
        "        batch_size: Batch size to return from iterator\n",
        "        root: Dataset root directory\n",
        "        train_file: Train filename\n",
        "        validation_file: Validation filename\n",
        "        test_file: Test filename\n",
        "        convert_digits: If True will convert numbers to single 0's\n",
        "    Returns:\n",
        "        A dict containing:\n",
        "            task: 'conll2003.' + tag_type\n",
        "            iters: (train iter, validation iter, test iter)\n",
        "            vocabs: (Inputs word vocabulary, Inputs character vocabulary, \n",
        "                    Tag vocabulary )\n",
        "    \"\"\"\n",
        "    \n",
        "    # Setup fields with batch dimension first\n",
        "    inputs_word = data.Field(  lower=True)\n",
        "\n",
        "    inputs_char_nesting = data.Field(tokenize=list)\n",
        "\n",
        "    inputs_char = data.NestedField(inputs_char_nesting)\n",
        "                        \n",
        "\n",
        "    labels = data.Field(unk_token = None, is_target=True)\n",
        "\n",
        "    fields = ([(('inputs_word', 'inputs_char'), (inputs_word, inputs_char))] + \n",
        "                [('labels', labels) if label == tag_type else (None, None) \n",
        "                for label in ['pos', 'chunk', 'ner']])\n",
        "\n",
        "    # Load the data\n",
        "    train, val, test = SequenceTaggingDataset.splits(\n",
        "                                path=root, \n",
        "                                train=train_file, \n",
        "                                validation=validation_file, \n",
        "                                test=test_file,\n",
        "                                separator=' ',\n",
        "                                fields=tuple(fields))\n",
        "\n",
        "\n",
        "    \n",
        "    # Build vocab\n",
        "    inputs_char.build_vocab(train.inputs_char, val.inputs_char, test.inputs_char)\n",
        "    inputs_word.build_vocab(train.inputs_word, val.inputs_word, test.inputs_word, max_size=50000,\n",
        "                        vectors= \"glove.6B.300d\")\n",
        "    \n",
        "    labels.build_vocab(train)\n",
        "    train_size = len(train)\n",
        "\n",
        "    # Get iterators\n",
        "    train_iter, val_iter, test_iter = data.BucketIterator.splits(\n",
        "                            (train, val, test), batch_size=batch_size, \n",
        "                            device=torch.device(\"cuda\"))\n",
        "    train_iter.repeat = False\n",
        "    \n",
        "    return {\n",
        "        'task': 'conll2003.%s'%tag_type,\n",
        "        'iters': (train_iter, val_iter, test_iter), \n",
        "        'vocabs': (inputs_word.vocab, inputs_char.vocab, labels.vocab), \n",
        "        'fields': (inputs_word,labels),\n",
        "        'size': train_size\n",
        "        }\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hC6qOKHrc5sN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#dic = conll2003_dataset('ner', batch_size = 256)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbTwNX2TPFUq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"a,_,_= dic['iters']\n",
        "\n",
        "for i,t in enumerate(a):\n",
        "  print(t)\n",
        "  if i == 1:\n",
        "    break\n",
        "\n",
        "  \"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TNnvi7LdubP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#d_,_,label_=dic['vocabs']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oME92UbOd6kE",
        "colab_type": "code",
        "outputId": "4c0a151b-13cc-4536-d35a-7d5679976ede",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#len(label_)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ly3jDoY6eHhn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c06d2c1e-fdbd-4800-ea43-0b6828807618"
      },
      "source": [
        "#label_.itos"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<pad>', 'O', 'I-PER', 'I-ORG', 'I-LOC', 'I-MISC', 'B-MISC', 'B-ORG', 'B-LOC']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ER3ElFk6ZocS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155
        },
        "outputId": "33302a89-bffd-400e-b408-c2505bd3b37b"
      },
      "source": [
        "#label_.freqs.most_common()"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('O', 170524),\n",
              " ('I-PER', 11128),\n",
              " ('I-ORG', 10001),\n",
              " ('I-LOC', 8286),\n",
              " ('I-MISC', 4556),\n",
              " ('B-MISC', 37),\n",
              " ('B-ORG', 24),\n",
              " ('B-LOC', 11)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5H9FyaZQw01",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class LSTM_Tagger(nn.Module):\n",
        "    def __init__(self, \n",
        "                 input_dim, \n",
        "                 embedding_dim, \n",
        "                 hidden_dim, \n",
        "                 output_dim, \n",
        "                 n_layers, \n",
        "                 dropout, \n",
        "                 pad_idx):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim, padding_idx = pad_idx)\n",
        "        \n",
        "        self.lstm = nn.LSTM(embedding_dim, \n",
        "                            hidden_dim, \n",
        "                            num_layers = n_layers,\n",
        "                            bidirectional = True,\n",
        "                            dropout = dropout if n_layers > 1 else 0)\n",
        "        \n",
        "        self.fc = nn.Linear(hidden_dim*2, output_dim)\n",
        "\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, text):\n",
        "\n",
        "        #text = [sent len, batch size]\n",
        "        \n",
        "        #pass text through embedding layer\n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "        \n",
        "        #embedded = [sent len, batch size, emb dim]\n",
        "        \n",
        "        #pass embeddings into LSTM\n",
        "        outputs, (hidden, cell) = self.lstm(embedded)\n",
        "        \n",
        "        #outputs holds the backward and forward hidden states in the final layer\n",
        "        #hidden and cell are the backward and forward hidden and cell states at the final time-step\n",
        "        \n",
        "        #output = [sent len, batch size, hid dim * n directions]\n",
        "        #hidden/cell = [n layers * n directions, batch size, hid dim]\n",
        "        \n",
        "        #we use our outputs to make a prediction of what the tag should be\n",
        "        predictions = self.fc(self.dropout(outputs))\n",
        "        \n",
        "        #predictions = [sent len, batch size, output dim]\n",
        "        \n",
        "        return predictions\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OptKt0ZWSuwE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bs=256\n",
        "dic = conll2003_dataset('ner', batch_size = bs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8oGzecjlZyr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_iter,valid_iter,test_iter = dic['iters']\n",
        "text_vocab,_,labels = dic['vocabs']\n",
        "input_field,label_field = dic['fields']\n",
        "train_size = dic['size']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6TA60cvTIHc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d29d5c6d-c4b6-4a28-b092-1bdb4fe9da71"
      },
      "source": [
        "train_size"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14987"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aqKhIL2oGHO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "INPUT_DIM = len(text_vocab)\n",
        "EMBEDDING_DIM = 300\n",
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = len(labels)\n",
        "N_LAYERS = 4\n",
        "\n",
        "DROPOUT = 0.25\n",
        "PAD_IDX = text_vocab.stoi[input_field.pad_token]\n",
        "model = LSTM_Tagger(INPUT_DIM, \n",
        "                        EMBEDDING_DIM, \n",
        "                        HIDDEN_DIM, \n",
        "                        OUTPUT_DIM, \n",
        "                        N_LAYERS,\n",
        "                        DROPOUT, \n",
        "                        PAD_IDX)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WThHQcnhX0YQ",
        "colab_type": "code",
        "outputId": "24bab6fe-7e06-4383-b8df-03b4d31c69e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pretrained_embeddings = text_vocab.vectors\n",
        "\n",
        "print(pretrained_embeddings.shape)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([26872, 300])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51cdRoCyX0YT",
        "colab_type": "code",
        "outputId": "c6fbc26b-b0e8-4e4f-a747-8df49e517bc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "source": [
        "model.embedding.weight.data.copy_(pretrained_embeddings)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0466,  0.2132, -0.0074,  ...,  0.0091, -0.2099,  0.0539],\n",
              "        ...,\n",
              "        [ 0.0702, -0.0498,  0.2675,  ...,  0.2944, -0.4164,  0.3202],\n",
              "        [-0.1881, -0.4332, -0.4754,  ...,  0.6583,  0.2311,  0.1486],\n",
              "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhr-6NRyX0YW",
        "colab_type": "code",
        "outputId": "7b9d4ce1-19d9-4784-acf8-bafebe596184",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "source": [
        "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "\n",
        "print(model.embedding.weight.data)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0466,  0.2132, -0.0074,  ...,  0.0091, -0.2099,  0.0539],\n",
            "        ...,\n",
            "        [ 0.0702, -0.0498,  0.2675,  ...,  0.2944, -0.4164,  0.3202],\n",
            "        [-0.1881, -0.4332, -0.4754,  ...,  0.6583,  0.2311,  0.1486],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsS_V0Ocooff",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda\")\n",
        "N_EPOCHS = 200\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqvjwF894_fc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
        "optimizer_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
        "]\n",
        "\n",
        "num_train_steps = int(train_size / bs * N_EPOCHS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jd_2AkSX0YZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = torch.optim.Adam(optimizer_parameters,lr = 2e-6)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqaIZfPuSDS5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma = 0.85)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lzxy1i2YX0Yc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TAG_PAD_IDX = labels.stoi[label_field.pad_token]\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rw_Ku0uiX0Yf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2c1utZRegOq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss(self, Y_hat, Y, X_lengths):\n",
        "      # TRICK 3 ********************************\n",
        "      # before we calculate the negative log likelihood, we need to mask out the activations\n",
        "      # this means we don't want to take into account padded items in the output vector\n",
        "      # simplest way to think about this is to flatten ALL sequences into a REALLY long sequence\n",
        "      # and calculate the loss on that.\n",
        "\n",
        "      # flatten all the labels\n",
        "      Y = Y.view(-1)\n",
        "\n",
        "      # flatten all predictions\n",
        "      Y_hat = Y_hat.view(-1, self.nb_tags)\n",
        "\n",
        "      # create a mask by filtering out all tokens that ARE NOT the padding token\n",
        "      tag_pad_token = self.tags['<PAD>']\n",
        "      mask = (Y > tag_pad_token).float()\n",
        "\n",
        "      # count how many tokens we have\n",
        "      nb_tokens = int(torch.sum(mask).data[0])\n",
        "\n",
        "      # pick the values for the label and zero out the rest with the mask\n",
        "      Y_hat = Y_hat[range(Y_hat.shape[0]), Y] * mask\n",
        "\n",
        "      # compute cross entropy loss which ignores all <PAD> tokens\n",
        "      ce_loss = -torch.sum(Y_hat) / nb_tokens\n",
        "\n",
        "      return ce_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWBOvVsKX0Yi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def categorical_accuracy(preds, y, tag_pad_idx):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "    max_preds = preds.argmax(dim = 1, keepdim = True) # get the index of the max probability\n",
        "    non_pad_elements = (y != tag_pad_idx).nonzero()\n",
        "    correct = max_preds[non_pad_elements].squeeze(1).eq(y[non_pad_elements])\n",
        "    return correct.sum() / torch.FloatTensor([y[non_pad_elements].shape[0]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qxr7S_yvxC1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def metric_f1(preds, y, tag_pad_idx):\n",
        "  preds=preds.to('cpu')\n",
        "  y = y.to('cpu')\n",
        "  max_preds = preds.argmax(dim = 1, keepdim = True) # get the index of the max probability\n",
        "  non_pad_elements = (y != tag_pad_idx).nonzero()\n",
        "  shape = max_preds[non_pad_elements].shape[0]\n",
        "  #print(max_preds[non_pad_elements])\n",
        "  #print(max_preds[non_pad_elements].view(shape).shape)\n",
        "  #print(y[non_pad_elements])\n",
        "  #print(y[non_pad_elements].view(shape).shape)\n",
        "  return f1_score(y[non_pad_elements].detach().view(shape).numpy(),max_preds[non_pad_elements].detach().view(shape).numpy(),average='macro')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsX6rKVBX0Yl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clip_gradient(model, clip_value):\n",
        "    params = list(filter(lambda p: p.grad is not None, model.parameters()))\n",
        "    for p in params:\n",
        "        p.grad.data.clamp_(-clip_value, clip_value)\n",
        "\n",
        "def train(model, iterator, optimizer,scheduler, criterion, tag_pad_idx):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    epoch_f1 = 0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for batch in iterator:\n",
        "        \n",
        "        text = batch.inputs_word\n",
        "        tags = batch.labels\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        #text = [sent len, batch size]\n",
        "        \n",
        "        predictions = model(text)\n",
        "        \n",
        "        #predictions = [sent len, batch size, output dim]\n",
        "        #tags = [sent len, batch size]\n",
        "        \n",
        "        predictions = predictions.view(-1, predictions.shape[-1])\n",
        "        tags = tags.view(-1)\n",
        "        \n",
        "        #predictions = [sent len * batch size, output dim]\n",
        "        #tags = [sent len * batch size]\n",
        "        \n",
        "        loss = criterion(predictions, tags)\n",
        "                \n",
        "        acc = categorical_accuracy(predictions, tags, tag_pad_idx)\n",
        "        f1 = metric_f1(predictions, tags, tag_pad_idx)\n",
        "        \n",
        "        loss.backward()\n",
        "        clip_gradient(model, 1e-1)\n",
        "        optimizer.step()\n",
        "        #scheduler.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "        epoch_f1 += f1\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator), epoch_f1 / len(iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHLt5qCwX0Yo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, iterator, criterion, tag_pad_idx):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    epoch_f1 = 0\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "\n",
        "            text = batch.inputs_word\n",
        "            tags = batch.labels\n",
        "            \n",
        "            predictions = model(text)\n",
        "            \n",
        "            predictions = predictions.view(-1, predictions.shape[-1])\n",
        "            tags = tags.view(-1)\n",
        "            \n",
        "            loss = criterion(predictions, tags)\n",
        "            \n",
        "            acc = categorical_accuracy(predictions, tags, tag_pad_idx)\n",
        "            f1 = metric_f1(predictions, tags, tag_pad_idx)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "            epoch_f1 += f1\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator), epoch_f1 / len(iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJA1lbSyX0Ys",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAFacwnuX0Yu",
        "colab_type": "code",
        "outputId": "0603b6ee-d34c-4ae9-f1d0-29d6d99b29a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "\n",
        "best_valid_f1 = float(0)\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss, train_acc, train_f1 = train(model, train_iter, optimizer, scheduler, criterion, TAG_PAD_IDX)\n",
        "    valid_loss, valid_acc, valid_f1 = evaluate(model, valid_iter, criterion, TAG_PAD_IDX)\n",
        "    \n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_f1 > best_valid_f1:\n",
        "        best_valid_f1 = valid_f1\n",
        "        torch.save(model.state_dict(), 'best_f1_bilstmmodel.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% | Train_F1: {train_f1}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}% | Valid_F1: {valid_f1}')"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.226 | Train Acc: 93.53% | Train_F1: 0.683882329284408\n",
            "\t Val. Loss: 0.196 |  Val. Acc: 93.47% | Valid_F1: 0.7747844727166319\n",
            "Epoch: 02 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.134 | Train Acc: 96.02% | Train_F1: 0.7674657034253424\n",
            "\t Val. Loss: 0.186 |  Val. Acc: 94.27% | Valid_F1: 0.7910886142169534\n",
            "Epoch: 03 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.132 | Train Acc: 96.15% | Train_F1: 0.7633085657492249\n",
            "\t Val. Loss: 0.179 |  Val. Acc: 94.51% | Valid_F1: 0.7778938480244408\n",
            "Epoch: 04 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.131 | Train Acc: 96.10% | Train_F1: 0.7601238801304604\n",
            "\t Val. Loss: 0.196 |  Val. Acc: 93.99% | Valid_F1: 0.7676471191812805\n",
            "Epoch: 05 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.132 | Train Acc: 96.10% | Train_F1: 0.7579168828041631\n",
            "\t Val. Loss: 0.190 |  Val. Acc: 94.41% | Valid_F1: 0.7796920526862786\n",
            "Epoch: 06 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.127 | Train Acc: 96.23% | Train_F1: 0.7653856933848678\n",
            "\t Val. Loss: 0.193 |  Val. Acc: 94.18% | Valid_F1: 0.7903060383403349\n",
            "Epoch: 07 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.128 | Train Acc: 96.19% | Train_F1: 0.7645013866010424\n",
            "\t Val. Loss: 0.189 |  Val. Acc: 94.40% | Valid_F1: 0.7865001598499768\n",
            "Epoch: 08 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.126 | Train Acc: 96.28% | Train_F1: 0.7640240130961558\n",
            "\t Val. Loss: 0.196 |  Val. Acc: 94.13% | Valid_F1: 0.7842207926009671\n",
            "Epoch: 09 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.128 | Train Acc: 96.24% | Train_F1: 0.7758622502825006\n",
            "\t Val. Loss: 0.191 |  Val. Acc: 94.16% | Valid_F1: 0.7718394940656624\n",
            "Epoch: 10 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.131 | Train Acc: 96.14% | Train_F1: 0.7549838666134574\n",
            "\t Val. Loss: 0.198 |  Val. Acc: 94.11% | Valid_F1: 0.7665913315266809\n",
            "Epoch: 11 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.128 | Train Acc: 96.21% | Train_F1: 0.7595784767866293\n",
            "\t Val. Loss: 0.199 |  Val. Acc: 94.08% | Valid_F1: 0.7741891955983117\n",
            "Epoch: 12 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.130 | Train Acc: 96.17% | Train_F1: 0.7658986850816729\n",
            "\t Val. Loss: 0.207 |  Val. Acc: 93.47% | Valid_F1: 0.7689823389809805\n",
            "Epoch: 13 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.128 | Train Acc: 96.23% | Train_F1: 0.7642403765497566\n",
            "\t Val. Loss: 0.212 |  Val. Acc: 93.48% | Valid_F1: 0.7620160703821977\n",
            "Epoch: 14 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.126 | Train Acc: 96.26% | Train_F1: 0.7711425378785558\n",
            "\t Val. Loss: 0.197 |  Val. Acc: 94.10% | Valid_F1: 0.7822070487798932\n",
            "Epoch: 15 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.127 | Train Acc: 96.25% | Train_F1: 0.7610474150975427\n",
            "\t Val. Loss: 0.201 |  Val. Acc: 94.09% | Valid_F1: 0.7773069163881993\n",
            "Epoch: 16 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.128 | Train Acc: 96.27% | Train_F1: 0.7735581134487907\n",
            "\t Val. Loss: 0.200 |  Val. Acc: 94.19% | Valid_F1: 0.7682258953514591\n",
            "Epoch: 17 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.126 | Train Acc: 96.27% | Train_F1: 0.7666623277639104\n",
            "\t Val. Loss: 0.199 |  Val. Acc: 94.11% | Valid_F1: 0.7771011436492915\n",
            "Epoch: 18 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.127 | Train Acc: 96.24% | Train_F1: 0.7621027945167103\n",
            "\t Val. Loss: 0.201 |  Val. Acc: 94.02% | Valid_F1: 0.7665387287685501\n",
            "Epoch: 19 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.128 | Train Acc: 96.17% | Train_F1: 0.7736847214553126\n",
            "\t Val. Loss: 0.181 |  Val. Acc: 94.83% | Valid_F1: 0.7887774341393902\n",
            "Epoch: 20 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.126 | Train Acc: 96.31% | Train_F1: 0.7706808850868963\n",
            "\t Val. Loss: 0.188 |  Val. Acc: 94.28% | Valid_F1: 0.7827379209650746\n",
            "Epoch: 21 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.129 | Train Acc: 96.18% | Train_F1: 0.7571556430741843\n",
            "\t Val. Loss: 0.194 |  Val. Acc: 94.17% | Valid_F1: 0.7826221244298545\n",
            "Epoch: 22 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.127 | Train Acc: 96.29% | Train_F1: 0.7681140223267894\n",
            "\t Val. Loss: 0.199 |  Val. Acc: 94.01% | Valid_F1: 0.7890478126964425\n",
            "Epoch: 23 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.127 | Train Acc: 96.17% | Train_F1: 0.7642257354930712\n",
            "\t Val. Loss: 0.210 |  Val. Acc: 94.21% | Valid_F1: 0.7765625920055083\n",
            "Epoch: 24 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.130 | Train Acc: 96.15% | Train_F1: 0.761946234586629\n",
            "\t Val. Loss: 0.181 |  Val. Acc: 94.67% | Valid_F1: 0.7880067250401671\n",
            "Epoch: 25 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.129 | Train Acc: 96.17% | Train_F1: 0.7597442145102637\n",
            "\t Val. Loss: 0.174 |  Val. Acc: 94.97% | Valid_F1: 0.7890618806018903\n",
            "Epoch: 26 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.127 | Train Acc: 96.30% | Train_F1: 0.7702766418625822\n",
            "\t Val. Loss: 0.188 |  Val. Acc: 94.18% | Valid_F1: 0.7743030520805092\n",
            "Epoch: 27 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.129 | Train Acc: 96.23% | Train_F1: 0.7563220213874453\n",
            "\t Val. Loss: 0.207 |  Val. Acc: 94.19% | Valid_F1: 0.7700477150814242\n",
            "Epoch: 28 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.131 | Train Acc: 96.13% | Train_F1: 0.7632284387895151\n",
            "\t Val. Loss: 0.182 |  Val. Acc: 94.87% | Valid_F1: 0.7825069403308162\n",
            "Epoch: 29 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.126 | Train Acc: 96.30% | Train_F1: 0.770586712651635\n",
            "\t Val. Loss: 0.184 |  Val. Acc: 94.36% | Valid_F1: 0.7845545257944433\n",
            "Epoch: 30 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.128 | Train Acc: 96.24% | Train_F1: 0.7631602054339229\n",
            "\t Val. Loss: 0.192 |  Val. Acc: 94.39% | Valid_F1: 0.7753458820248976\n",
            "Epoch: 31 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.128 | Train Acc: 96.23% | Train_F1: 0.7727696134626829\n",
            "\t Val. Loss: 0.191 |  Val. Acc: 94.61% | Valid_F1: 0.7846175636614691\n",
            "Epoch: 32 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.127 | Train Acc: 96.27% | Train_F1: 0.766081174487114\n",
            "\t Val. Loss: 0.213 |  Val. Acc: 93.55% | Valid_F1: 0.7625744366997669\n",
            "Epoch: 33 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.128 | Train Acc: 96.24% | Train_F1: 0.7592618662368138\n",
            "\t Val. Loss: 0.199 |  Val. Acc: 94.15% | Valid_F1: 0.7773985384542793\n",
            "Epoch: 34 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.128 | Train Acc: 96.22% | Train_F1: 0.767072561807856\n",
            "\t Val. Loss: 0.185 |  Val. Acc: 94.64% | Valid_F1: 0.7836803711301289\n",
            "Epoch: 35 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.129 | Train Acc: 96.23% | Train_F1: 0.7666322124235074\n",
            "\t Val. Loss: 0.185 |  Val. Acc: 94.69% | Valid_F1: 0.7869181358473255\n",
            "Epoch: 36 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.130 | Train Acc: 96.18% | Train_F1: 0.7721169627925085\n",
            "\t Val. Loss: 0.194 |  Val. Acc: 94.16% | Valid_F1: 0.7715307119771744\n",
            "Epoch: 37 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.129 | Train Acc: 96.17% | Train_F1: 0.7710649588339304\n",
            "\t Val. Loss: 0.187 |  Val. Acc: 94.57% | Valid_F1: 0.7833967545255801\n",
            "Epoch: 38 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.129 | Train Acc: 96.17% | Train_F1: 0.764408316412659\n",
            "\t Val. Loss: 0.201 |  Val. Acc: 94.29% | Valid_F1: 0.773202686398297\n",
            "Epoch: 39 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.129 | Train Acc: 96.17% | Train_F1: 0.7687444167270614\n",
            "\t Val. Loss: 0.194 |  Val. Acc: 94.56% | Valid_F1: 0.7929037737534365\n",
            "Epoch: 40 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.127 | Train Acc: 96.26% | Train_F1: 0.7702261120099464\n",
            "\t Val. Loss: 0.197 |  Val. Acc: 94.41% | Valid_F1: 0.7835209553836278\n",
            "Epoch: 41 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.126 | Train Acc: 96.33% | Train_F1: 0.7683535695581517\n",
            "\t Val. Loss: 0.190 |  Val. Acc: 94.68% | Valid_F1: 0.7844392008683077\n",
            "Epoch: 42 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.126 | Train Acc: 96.28% | Train_F1: 0.7652680717505657\n",
            "\t Val. Loss: 0.201 |  Val. Acc: 93.49% | Valid_F1: 0.7746582854747943\n",
            "Epoch: 43 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.130 | Train Acc: 96.18% | Train_F1: 0.7717824741344498\n",
            "\t Val. Loss: 0.202 |  Val. Acc: 94.10% | Valid_F1: 0.7744692624489007\n",
            "Epoch: 44 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.128 | Train Acc: 96.21% | Train_F1: 0.769232207562054\n",
            "\t Val. Loss: 0.203 |  Val. Acc: 93.78% | Valid_F1: 0.7845216118992735\n",
            "Epoch: 45 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.127 | Train Acc: 96.20% | Train_F1: 0.7717052396508075\n",
            "\t Val. Loss: 0.262 |  Val. Acc: 90.27% | Valid_F1: 0.7359015401096393\n",
            "Epoch: 46 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.127 | Train Acc: 96.25% | Train_F1: 0.7709103952806073\n",
            "\t Val. Loss: 0.218 |  Val. Acc: 92.93% | Valid_F1: 0.7726322937372895\n",
            "Epoch: 47 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.130 | Train Acc: 96.12% | Train_F1: 0.7679590017884643\n",
            "\t Val. Loss: 0.222 |  Val. Acc: 92.70% | Valid_F1: 0.7591384210362501\n",
            "Epoch: 48 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.128 | Train Acc: 96.17% | Train_F1: 0.7609223294218176\n",
            "\t Val. Loss: 0.228 |  Val. Acc: 92.16% | Valid_F1: 0.7693703116038447\n",
            "Epoch: 49 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.130 | Train Acc: 96.15% | Train_F1: 0.7666822057527389\n",
            "\t Val. Loss: 0.226 |  Val. Acc: 92.57% | Valid_F1: 0.7691442542341134\n",
            "Epoch: 50 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.129 | Train Acc: 96.16% | Train_F1: 0.7615840483004855\n",
            "\t Val. Loss: 0.225 |  Val. Acc: 93.57% | Valid_F1: 0.7597481128172935\n",
            "Epoch: 51 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.126 | Train Acc: 96.28% | Train_F1: 0.7669954522426509\n",
            "\t Val. Loss: 0.206 |  Val. Acc: 93.30% | Valid_F1: 0.7754765000492089\n",
            "Epoch: 52 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.128 | Train Acc: 96.26% | Train_F1: 0.772359333059549\n",
            "\t Val. Loss: 0.223 |  Val. Acc: 92.72% | Valid_F1: 0.7770174934371571\n",
            "Epoch: 53 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.128 | Train Acc: 96.22% | Train_F1: 0.7706059854040164\n",
            "\t Val. Loss: 0.212 |  Val. Acc: 92.98% | Valid_F1: 0.7671129536779825\n",
            "Epoch: 54 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.126 | Train Acc: 96.31% | Train_F1: 0.7725559361567396\n",
            "\t Val. Loss: 0.231 |  Val. Acc: 91.98% | Valid_F1: 0.7655137125199288\n",
            "Epoch: 55 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.130 | Train Acc: 96.15% | Train_F1: 0.7640527857266031\n",
            "\t Val. Loss: 0.235 |  Val. Acc: 91.77% | Valid_F1: 0.7618817051369524\n",
            "Epoch: 56 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.126 | Train Acc: 96.27% | Train_F1: 0.7766331751734371\n",
            "\t Val. Loss: 0.243 |  Val. Acc: 91.60% | Valid_F1: 0.7555181677677411\n",
            "Epoch: 57 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.130 | Train Acc: 96.12% | Train_F1: 0.7483101108647926\n",
            "\t Val. Loss: 0.234 |  Val. Acc: 91.91% | Valid_F1: 0.7636750490526797\n",
            "Epoch: 58 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.129 | Train Acc: 96.23% | Train_F1: 0.7638952212654301\n",
            "\t Val. Loss: 0.237 |  Val. Acc: 92.46% | Valid_F1: 0.7685902665034785\n",
            "Epoch: 59 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.129 | Train Acc: 96.15% | Train_F1: 0.7608371033449484\n",
            "\t Val. Loss: 0.237 |  Val. Acc: 91.99% | Valid_F1: 0.7618900690818288\n",
            "Epoch: 60 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.131 | Train Acc: 96.14% | Train_F1: 0.7665866771707726\n",
            "\t Val. Loss: 0.231 |  Val. Acc: 91.95% | Valid_F1: 0.7707086453573468\n",
            "Epoch: 61 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.127 | Train Acc: 96.23% | Train_F1: 0.7721168115618008\n",
            "\t Val. Loss: 0.227 |  Val. Acc: 93.06% | Valid_F1: 0.7764563021969586\n",
            "Epoch: 62 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.128 | Train Acc: 96.21% | Train_F1: 0.7716707505203898\n",
            "\t Val. Loss: 0.209 |  Val. Acc: 92.81% | Valid_F1: 0.7690301164527905\n",
            "Epoch: 63 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.128 | Train Acc: 96.19% | Train_F1: 0.7659796497529863\n",
            "\t Val. Loss: 0.262 |  Val. Acc: 90.97% | Valid_F1: 0.7572029343388742\n",
            "Epoch: 64 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.129 | Train Acc: 96.19% | Train_F1: 0.7747230371311522\n",
            "\t Val. Loss: 0.239 |  Val. Acc: 91.77% | Valid_F1: 0.7631719437363954\n",
            "Epoch: 65 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.129 | Train Acc: 96.21% | Train_F1: 0.7671272561764195\n",
            "\t Val. Loss: 0.241 |  Val. Acc: 92.37% | Valid_F1: 0.7596090797827902\n",
            "Epoch: 66 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.128 | Train Acc: 96.20% | Train_F1: 0.7645940079891377\n",
            "\t Val. Loss: 0.252 |  Val. Acc: 91.61% | Valid_F1: 0.7543772596636059\n",
            "Epoch: 67 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.128 | Train Acc: 96.17% | Train_F1: 0.7542693249831536\n",
            "\t Val. Loss: 0.236 |  Val. Acc: 92.37% | Valid_F1: 0.7668858194715714\n",
            "Epoch: 68 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.126 | Train Acc: 96.26% | Train_F1: 0.7696339398951576\n",
            "\t Val. Loss: 0.205 |  Val. Acc: 93.55% | Valid_F1: 0.782681916066401\n",
            "Epoch: 69 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.126 | Train Acc: 96.25% | Train_F1: 0.7612057173679351\n",
            "\t Val. Loss: 0.231 |  Val. Acc: 92.44% | Valid_F1: 0.770905037169726\n",
            "Epoch: 70 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.129 | Train Acc: 96.22% | Train_F1: 0.7655619506945358\n",
            "\t Val. Loss: 0.227 |  Val. Acc: 92.51% | Valid_F1: 0.774404051326836\n",
            "Epoch: 71 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.128 | Train Acc: 96.16% | Train_F1: 0.764481191636568\n",
            "\t Val. Loss: 0.259 |  Val. Acc: 91.27% | Valid_F1: 0.7571983195240485\n",
            "Epoch: 72 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.128 | Train Acc: 96.26% | Train_F1: 0.769076422367269\n",
            "\t Val. Loss: 0.276 |  Val. Acc: 90.65% | Valid_F1: 0.75382422052886\n",
            "Epoch: 73 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.130 | Train Acc: 96.13% | Train_F1: 0.7655445063857906\n",
            "\t Val. Loss: 0.254 |  Val. Acc: 91.33% | Valid_F1: 0.7500495605760572\n",
            "Epoch: 74 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.123 | Train Acc: 96.36% | Train_F1: 0.7731687491981375\n",
            "\t Val. Loss: 0.279 |  Val. Acc: 91.26% | Valid_F1: 0.7538054057152882\n",
            "Epoch: 75 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.129 | Train Acc: 96.18% | Train_F1: 0.7639284169883765\n",
            "\t Val. Loss: 0.234 |  Val. Acc: 92.40% | Valid_F1: 0.7598521820583778\n",
            "Epoch: 76 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.130 | Train Acc: 96.13% | Train_F1: 0.760847866670619\n",
            "\t Val. Loss: 0.235 |  Val. Acc: 92.35% | Valid_F1: 0.7661987723285134\n",
            "Epoch: 77 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.125 | Train Acc: 96.29% | Train_F1: 0.7650023037383955\n",
            "\t Val. Loss: 0.259 |  Val. Acc: 91.41% | Valid_F1: 0.7544156588071834\n",
            "Epoch: 78 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.129 | Train Acc: 96.15% | Train_F1: 0.7620765414972249\n",
            "\t Val. Loss: 0.294 |  Val. Acc: 89.89% | Valid_F1: 0.7332247112138216\n",
            "Epoch: 79 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.128 | Train Acc: 96.25% | Train_F1: 0.767905820952542\n",
            "\t Val. Loss: 0.278 |  Val. Acc: 90.17% | Valid_F1: 0.7386044767302299\n",
            "Epoch: 80 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.129 | Train Acc: 96.18% | Train_F1: 0.7650322248531314\n",
            "\t Val. Loss: 0.311 |  Val. Acc: 89.50% | Valid_F1: 0.7438258124134512\n",
            "Epoch: 81 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.130 | Train Acc: 96.19% | Train_F1: 0.7672619772376915\n",
            "\t Val. Loss: 0.270 |  Val. Acc: 91.44% | Valid_F1: 0.7579032361293924\n",
            "Epoch: 82 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.128 | Train Acc: 96.22% | Train_F1: 0.7553462030254863\n",
            "\t Val. Loss: 0.273 |  Val. Acc: 91.45% | Valid_F1: 0.752131580501569\n",
            "Epoch: 83 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.127 | Train Acc: 96.24% | Train_F1: 0.7711721327176944\n",
            "\t Val. Loss: 0.302 |  Val. Acc: 89.44% | Valid_F1: 0.733717946449529\n",
            "Epoch: 84 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.125 | Train Acc: 96.29% | Train_F1: 0.7739385091202735\n",
            "\t Val. Loss: 0.267 |  Val. Acc: 91.51% | Valid_F1: 0.7602522219260424\n",
            "Epoch: 85 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.127 | Train Acc: 96.24% | Train_F1: 0.7713328500433664\n",
            "\t Val. Loss: 0.292 |  Val. Acc: 89.90% | Valid_F1: 0.7348886459957795\n",
            "Epoch: 86 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.129 | Train Acc: 96.17% | Train_F1: 0.7628717685029708\n",
            "\t Val. Loss: 0.313 |  Val. Acc: 90.10% | Valid_F1: 0.7314021297605408\n",
            "Epoch: 87 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.128 | Train Acc: 96.18% | Train_F1: 0.7576524717041565\n",
            "\t Val. Loss: 0.279 |  Val. Acc: 91.39% | Valid_F1: 0.7588948343212376\n",
            "Epoch: 88 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.126 | Train Acc: 96.27% | Train_F1: 0.7612970570218068\n",
            "\t Val. Loss: 0.357 |  Val. Acc: 89.09% | Valid_F1: 0.725934182603507\n",
            "Epoch: 89 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.127 | Train Acc: 96.27% | Train_F1: 0.772794044787924\n",
            "\t Val. Loss: 0.343 |  Val. Acc: 88.88% | Valid_F1: 0.7298965768015926\n",
            "Epoch: 90 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.129 | Train Acc: 96.17% | Train_F1: 0.7684191140823353\n",
            "\t Val. Loss: 0.289 |  Val. Acc: 90.50% | Valid_F1: 0.7430265746700911\n",
            "Epoch: 91 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.128 | Train Acc: 96.17% | Train_F1: 0.7705219517841813\n",
            "\t Val. Loss: 0.297 |  Val. Acc: 90.13% | Valid_F1: 0.7508756806637065\n",
            "Epoch: 92 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.128 | Train Acc: 96.19% | Train_F1: 0.759767831965385\n",
            "\t Val. Loss: 0.346 |  Val. Acc: 88.97% | Valid_F1: 0.7304894887818911\n",
            "Epoch: 93 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.127 | Train Acc: 96.25% | Train_F1: 0.7662316979102627\n",
            "\t Val. Loss: 0.292 |  Val. Acc: 91.05% | Valid_F1: 0.7525379113978533\n",
            "Epoch: 94 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.127 | Train Acc: 96.21% | Train_F1: 0.7614080894729706\n",
            "\t Val. Loss: 0.280 |  Val. Acc: 91.36% | Valid_F1: 0.7535319732603613\n",
            "Epoch: 95 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.126 | Train Acc: 96.28% | Train_F1: 0.7619764201797453\n",
            "\t Val. Loss: 0.298 |  Val. Acc: 90.55% | Valid_F1: 0.7395185614314862\n",
            "Epoch: 96 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.127 | Train Acc: 96.21% | Train_F1: 0.763057140331843\n",
            "\t Val. Loss: 0.309 |  Val. Acc: 89.69% | Valid_F1: 0.742899669976304\n",
            "Epoch: 97 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.129 | Train Acc: 96.20% | Train_F1: 0.7582775194088601\n",
            "\t Val. Loss: 0.291 |  Val. Acc: 91.03% | Valid_F1: 0.7474946074047272\n",
            "Epoch: 98 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.128 | Train Acc: 96.21% | Train_F1: 0.7691618198141656\n",
            "\t Val. Loss: 0.278 |  Val. Acc: 91.31% | Valid_F1: 0.76070685521616\n",
            "Epoch: 99 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.128 | Train Acc: 96.22% | Train_F1: 0.7672328288726942\n",
            "\t Val. Loss: 0.299 |  Val. Acc: 90.10% | Valid_F1: 0.7476369930506044\n",
            "Epoch: 100 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.127 | Train Acc: 96.26% | Train_F1: 0.765678401406861\n",
            "\t Val. Loss: 0.263 |  Val. Acc: 90.77% | Valid_F1: 0.7425078155565975\n",
            "Epoch: 101 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.128 | Train Acc: 96.19% | Train_F1: 0.7595113329371055\n",
            "\t Val. Loss: 0.328 |  Val. Acc: 88.82% | Valid_F1: 0.7300458177208885\n",
            "Epoch: 102 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.128 | Train Acc: 96.21% | Train_F1: 0.7630100178629141\n",
            "\t Val. Loss: 0.290 |  Val. Acc: 90.39% | Valid_F1: 0.7413587102079218\n",
            "Epoch: 103 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.126 | Train Acc: 96.26% | Train_F1: 0.7680496008369556\n",
            "\t Val. Loss: 0.316 |  Val. Acc: 89.42% | Valid_F1: 0.7383354978328697\n",
            "Epoch: 104 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.125 | Train Acc: 96.27% | Train_F1: 0.7570544958837565\n",
            "\t Val. Loss: 0.335 |  Val. Acc: 88.49% | Valid_F1: 0.7307855101745265\n",
            "Epoch: 105 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.128 | Train Acc: 96.16% | Train_F1: 0.7652812543412434\n",
            "\t Val. Loss: 0.260 |  Val. Acc: 91.30% | Valid_F1: 0.7479690613984313\n",
            "Epoch: 106 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.126 | Train Acc: 96.28% | Train_F1: 0.7696152797277478\n",
            "\t Val. Loss: 0.319 |  Val. Acc: 89.68% | Valid_F1: 0.7458159132676178\n",
            "Epoch: 107 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.124 | Train Acc: 96.35% | Train_F1: 0.7724668539589138\n",
            "\t Val. Loss: 0.300 |  Val. Acc: 90.88% | Valid_F1: 0.7438430525523303\n",
            "Epoch: 108 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.130 | Train Acc: 96.17% | Train_F1: 0.7579346195570583\n",
            "\t Val. Loss: 0.307 |  Val. Acc: 89.77% | Valid_F1: 0.7370685807802226\n",
            "Epoch: 109 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.128 | Train Acc: 96.22% | Train_F1: 0.7643224567051329\n",
            "\t Val. Loss: 0.381 |  Val. Acc: 87.91% | Valid_F1: 0.7197990711770558\n",
            "Epoch: 110 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.128 | Train Acc: 96.20% | Train_F1: 0.7609419419239504\n",
            "\t Val. Loss: 0.302 |  Val. Acc: 91.28% | Valid_F1: 0.7529605321603466\n",
            "Epoch: 111 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.124 | Train Acc: 96.39% | Train_F1: 0.7702991270721483\n",
            "\t Val. Loss: 0.362 |  Val. Acc: 88.26% | Valid_F1: 0.7290324927103573\n",
            "Epoch: 112 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.128 | Train Acc: 96.16% | Train_F1: 0.7561327140158554\n",
            "\t Val. Loss: 0.337 |  Val. Acc: 89.57% | Valid_F1: 0.7452144866517842\n",
            "Epoch: 113 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.127 | Train Acc: 96.21% | Train_F1: 0.7678504869224323\n",
            "\t Val. Loss: 0.277 |  Val. Acc: 91.31% | Valid_F1: 0.7433695595337898\n",
            "Epoch: 114 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.129 | Train Acc: 96.13% | Train_F1: 0.7635615646000612\n",
            "\t Val. Loss: 0.286 |  Val. Acc: 90.50% | Valid_F1: 0.7533562625272274\n",
            "Epoch: 115 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.125 | Train Acc: 96.30% | Train_F1: 0.7681930326328017\n",
            "\t Val. Loss: 0.341 |  Val. Acc: 88.81% | Valid_F1: 0.7351202409912492\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-104-e77d213d30f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_f1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTAG_PAD_IDX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_f1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTAG_PAD_IDX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-97-d822603ecbed>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, scheduler, criterion, tag_pad_idx)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs_word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchtext/data/iterator.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    155\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m                         \u001b[0mminibatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0mBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminibatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchtext/data/batch.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, dataset, device)\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mfield\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                     \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchtext/data/field.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, batch, device)\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mcustom\u001b[0m \u001b[0mpostprocessing\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \"\"\"\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0mpadded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumericalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchtext/data/field.py\u001b[0m in \u001b[0;36mpad\u001b[0;34m(self, minibatch)\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnesting_field\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minclude_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m         \u001b[0mpadded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNestedField\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminibatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 574\u001b[0;31m         \u001b[0mpadded_with_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnesting_field\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpadded\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    575\u001b[0m         \u001b[0mword_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0mfinal_padded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchtext/data/field.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnesting_field\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minclude_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m         \u001b[0mpadded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNestedField\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminibatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 574\u001b[0;31m         \u001b[0mpadded_with_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnesting_field\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpadded\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    575\u001b[0m         \u001b[0mword_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0mfinal_padded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchtext/data/field.py\u001b[0m in \u001b[0;36mpad\u001b[0;34m(self, minibatch)\u001b[0m\n\u001b[1;32m    232\u001b[0m                     \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_token\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_token\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m                     \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtruncate_first\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                     \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos_token\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos_token\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m                     [self.pad_token] * max(0, max_len - len(x)))\n\u001b[1;32m    236\u001b[0m             \u001b[0mlengths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnbNBGFoteFX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c5dd1547-da90-4713-814b-59589e2b4331"
      },
      "source": [
        "model.load_state_dict(torch.load('best_f1_bilstmmodel.pt'))\n",
        "\n",
        "test_loss, test_acc,test_f1 = evaluate(model, test_iter, criterion, TAG_PAD_IDX)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} |  Test Acc: {test_acc*100:.2f}% | Test_F1: {test_f1}')"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.276 |  Test Acc: 91.63% | Test_F1: 0.6696509067033248\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9kf6M7PteCC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IaMRwC5MX0Y1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tag_sentence(model, device, sentence, text_field, tag_field):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    if isinstance(sentence, str):\n",
        "        nlp = spacy.load('en')\n",
        "        tokens = [token.text for token in nlp(sentence)]\n",
        "    else:\n",
        "        tokens = [token for token in sentence]\n",
        "\n",
        "    if text_field.lower:\n",
        "        tokens = [t.lower() for t in tokens]\n",
        "        \n",
        "    numericalized_tokens = [text_field.vocab.stoi[t] for t in tokens]\n",
        "\n",
        "    unk_idx = text_field.vocab.stoi[text_field.unk_token]\n",
        "    \n",
        "    unks = [t for t, n in zip(tokens, numericalized_tokens) if n == unk_idx]\n",
        "    \n",
        "    token_tensor = torch.LongTensor(numericalized_tokens)\n",
        "    \n",
        "    token_tensor = token_tensor.unsqueeze(-1).to(device)\n",
        "         \n",
        "    predictions = model(token_tensor)\n",
        "    \n",
        "    top_predictions = predictions.argmax(-1)\n",
        "    \n",
        "    predicted_tags = [tag_field.vocab.itos[t.item()] for t in top_predictions]\n",
        "    \n",
        "    return tokens, predicted_tags, unks"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "johHmAKHX0Y3",
        "colab_type": "code",
        "outputId": "ac7c3208-b3dc-48ca-921c-fef8e54f3dde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "example_index = 1\n",
        "\n",
        "sentence = 'The Queen will deliver a speech about the conflict in North Korea at 1pm tomorrow.'\n",
        "\n",
        "\n",
        "print(sentence)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Queen will deliver a speech about the conflict in North Korea at 1pm tomorrow.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TlCvtpYX0Y6",
        "colab_type": "code",
        "outputId": "6f4181a7-05b3-451d-a056-fc7162d09b57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import spacy\n",
        "tokens, pred_tags, unks = tag_sentence(model, \n",
        "                                       device, \n",
        "                                       sentence, \n",
        "                                       input_field, \n",
        "                                       label_field)\n",
        "\n",
        "print(pred_tags)\n",
        "print(tokens)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-LOC', 'O', 'O', 'O', 'O', 'O']\n",
            "['the', 'queen', 'will', 'deliver', 'a', 'speech', 'about', 'the', 'conflict', 'in', 'north', 'korea', 'at', '1', 'pm', 'tomorrow', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1UqTJhnX0Y9",
        "colab_type": "code",
        "outputId": "f7b770c3-a59a-46bc-eae8-6761c05b7d29",
        "colab": {}
      },
      "source": [
        "print(\"Pred. Tag\\tActual Tag\\tCorrect?\\tToken\\n\")\n",
        "\n",
        "for token, pred_tag, actual_tag in zip(tokens, pred_tags, actual_tags):\n",
        "    correct = '✔' if pred_tag == actual_tag else '✘'\n",
        "    print(f\"{pred_tag}\\t\\t{actual_tag}\\t\\t{correct}\\t\\t{token}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pred. Tag\tActual Tag\tCorrect?\tToken\n",
            "\n",
            "PUNCT\t\tPUNCT\t\t✔\t\t[\n",
            "DET\t\tDET\t\t✔\t\tthis\n",
            "NOUN\t\tNOUN\t\t✔\t\tkilling\n",
            "ADP\t\tADP\t\t✔\t\tof\n",
            "DET\t\tDET\t\t✔\t\ta\n",
            "ADJ\t\tADJ\t\t✔\t\trespected\n",
            "NOUN\t\tNOUN\t\t✔\t\tcleric\n",
            "AUX\t\tAUX\t\t✔\t\twill\n",
            "AUX\t\tAUX\t\t✔\t\tbe\n",
            "VERB\t\tVERB\t\t✔\t\tcausing\n",
            "PRON\t\tPRON\t\t✔\t\tus\n",
            "NOUN\t\tNOUN\t\t✔\t\ttrouble\n",
            "ADP\t\tADP\t\t✔\t\tfor\n",
            "NOUN\t\tNOUN\t\t✔\t\tyears\n",
            "PART\t\tPART\t\t✔\t\tto\n",
            "VERB\t\tVERB\t\t✔\t\tcome\n",
            "PUNCT\t\tPUNCT\t\t✔\t\t.\n",
            "PUNCT\t\tPUNCT\t\t✔\t\t]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOQBmbWLX0Y_",
        "colab_type": "code",
        "outputId": "cdebf044-42f1-4713-b8f2-b2b781bf41bb",
        "colab": {}
      },
      "source": [
        "sentence = 'The Queen will deliver a speech about the conflict in North Korea at 1pm tomorrow.'\n",
        "\n",
        "tokens, tags, unks = tag_sentence(model, \n",
        "                                  device, \n",
        "                                  sentence, \n",
        "                                  TEXT, \n",
        "                                  UD_TAGS)\n",
        "\n",
        "print(unks)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wu9_5I4CX0ZC",
        "colab_type": "code",
        "outputId": "37486a25-fd87-4d9a-9f1f-0e72aefef369",
        "colab": {}
      },
      "source": [
        "print(\"Pred. Tag\\tToken\\n\")\n",
        "\n",
        "for token, tag in zip(tokens, tags):\n",
        "    print(f\"{tag}\\t\\t{token}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pred. Tag\tToken\n",
            "\n",
            "DET\t\tthe\n",
            "NOUN\t\tqueen\n",
            "AUX\t\twill\n",
            "VERB\t\tdeliver\n",
            "DET\t\ta\n",
            "NOUN\t\tspeech\n",
            "ADP\t\tabout\n",
            "DET\t\tthe\n",
            "NOUN\t\tconflict\n",
            "ADP\t\tin\n",
            "PROPN\t\tnorth\n",
            "PROPN\t\tkorea\n",
            "ADP\t\tat\n",
            "NUM\t\t1\n",
            "NOUN\t\tpm\n",
            "NOUN\t\ttomorrow\n",
            "PUNCT\t\t.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}